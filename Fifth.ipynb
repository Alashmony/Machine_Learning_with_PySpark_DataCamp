{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e00f1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width : 99% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.output_result {width : 99% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.jp-Notebook {--jp-notebook-max-width: 99%;}/style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container {width : 99% !important;}</style>\"))\n",
    "display(HTML(\"<style>.output_result {width : 99% !important;}</style>\"))\n",
    "display(HTML(\"<style>.jp-Notebook {--jp-notebook-max-width: 99%;}/style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f727b75-72b0-4c24-9d01-dbbae4653a6f",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1a0fd5-1d52-449a-8800-22803b063bb9",
   "metadata": {},
   "source": [
    "## Machine Learning & Spark\n",
    "1. Machine Learning & Spark\n",
    "\n",
    "Hi! Welcome to the course on Machine Learning with Apache Spark, in which you will learn how to build Machine Learning models on large data sets using distributed computing techniques. Let's start with some fundamental concepts.\n",
    "\n",
    "2. Building the perfect waffle (an analogy)\n",
    "\n",
    "Suppose you wanted to teach a computer how to make waffles. You could find a good recipe and then give the computer explicit instructions about ingredients and proportions. Alternatively, you could present the computer with a selection of different waffle recipes and let it figure out the ingredients and proportions for the best recipe. The second approach is how Machine Learning works: the computer literally learns from examples.\n",
    "\n",
    "3. Regression & classification\n",
    "\n",
    "Machine Learning problems are generally less esoteric than finding the perfect waffle recipe. The most common problems apply either Regression or Classification. A regression model learns to predict a number. For example, when making waffles, how much flour should be used for a particular amount of sugar? A classification model, on the other hand, predicts a discrete or categorical value. For example, is a recipe calling for a particular amount of sugar and salt more likely to be for waffles or cupcakes?\n",
    "\n",
    "4. Data in RAM\n",
    "\n",
    "The performance of a Machine Learning model depends on data. In general, more data is a good thing. If an algorithm is able to train on a larger set of data, then its ability to generalize to new data will inevitably improve. However, there are some practical constraints. If the data can fit entirely into RAM then the algorithm can operate efficiently. What happens when those data no longer fit into memory?\n",
    "\n",
    "5. Data exceeds RAM\n",
    "\n",
    "The computer will start to use *virtual memory* and data will be *paged* back and forth between RAM and disk. Relative to RAM access, retrieving data from disk is slow. As the size of the data grows, paging becomes more intense and the computer begins to spend more and more time waiting for data. Performance plummets.\n",
    "\n",
    "6. Data distributed across a cluster\n",
    "\n",
    "How then do we deal with truly large datasets? One option is to distribute the problem across multiple computers in a cluster. Rather than trying to handle a large dataset on a single machine, it's divided up into partitions which are processed separately. Ideally each data partition can fit into RAM on a single computer in the cluster. This is the approach used by Spark.\n",
    "\n",
    "7. What is Spark?\n",
    "\n",
    "Spark is a general purpose framework for cluster computing. It is popular for two main reasons: 1. it's generally much faster than other Big Data technologies like Hadoop, because it does most processing in memory and 2. it has a developer-friendly interface which hides much of the complexity of distributed computing.\n",
    "\n",
    "8. Components: nodes\n",
    "\n",
    "Let's review the components of a Spark cluster. The cluster itself consists of one or more nodes. Each node is a computer with CPU, RAM and physical storage.\n",
    "\n",
    "9. Components: cluster manager\n",
    "\n",
    "A cluster manager allocates resources and coordinates activity across the cluster.\n",
    "\n",
    "10. Components: driver\n",
    "\n",
    "Every application running on the Spark cluster has a driver program. Using the Spark API, the driver communicates with the cluster manager, which in turn distributes work to the nodes.\n",
    "\n",
    "11. Components: executors\n",
    "\n",
    "On each node Spark launches an executor process which persists for the duration of the application. Work is divided up into tasks, which are simply units of computation. The executors run tasks in multiple threads across the cores in a node. When working with Spark you normally don't need to worry *too* much about the details of the cluster. Spark sets up all of that infrastructure for you and handles all interactions within the cluster. However, it's still useful to know how it works under the hood.\n",
    "\n",
    "12. Onward!\n",
    "\n",
    "You now have a basic understanding of the principles of Machine Learning and distributed computing with Spark. Next we'll learn how to connect to a Spark cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676fee60-2721-422f-bde5-f2d260023df0",
   "metadata": {},
   "source": [
    "### Characteristics of Spark\n",
    "Spark is currently the most popular technology for processing large quantities of data. Not only is it able to handle enormous data volumes, but it does so very efficiently too! Also, unlike some other distributed computing technologies, developing with Spark is a pleasure.\n",
    "\n",
    "Which of these describes Spark?\n",
    "\n",
    "Answer the question\n",
    "\n",
    "\n",
    "- Spark is a framework for cluster computing.\n",
    "\n",
    "- Spark does most processing in memory.\n",
    "\n",
    "- Spark has a high-level API, which conceals a lot of complexity.\n",
    "\n",
    "- **All of the above.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f6273-f177-4e3d-bbda-b26b1646122f",
   "metadata": {},
   "source": [
    "### Components in a Spark Cluster\n",
    "Spark is a distributed computing platform. It achieves efficiency by distributing data and computation across a cluster of computers.\n",
    "\n",
    "A Spark cluster consists of a number of hardware and software components which work together.\n",
    "\n",
    "Which of these is not part of a Spark cluster?\n",
    "\n",
    "Answer the question\n",
    "\n",
    "- One or more nodes\n",
    "\n",
    "- A cluster manager\n",
    "\n",
    "- **A load balancer**\n",
    "\n",
    "- Executors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6af067-2fc8-42fc-914d-89a1e9f2e956",
   "metadata": {},
   "source": [
    "## Connecting to Spark\n",
    "1. Connecting to Spark\n",
    "\n",
    "The previous lesson was high level overviews of Machine Learning and Spark. In this lesson you'll review the process of connecting to Spark.\n",
    "\n",
    "2. Interacting with Spark\n",
    "\n",
    "The connection with Spark is established by the driver, which can be written in either Java, Scala, Python or R. Each of these languages has advantages and disadvantages. Java is relatively verbose, requiring a lot of code to accomplish even simple tasks. By contrast, Scala, Python and R, are high-level languages which can accomplish much with only a small amount of code. They also offer a REPL, or Read-Evaluate-Print loop, which is crucial for interactive development. You'll be using Python.\n",
    "\n",
    "3. Importing pyspark\n",
    "\n",
    "Python doesn't talk natively to Spark, so we'll kick off by importing the pyspark module, which makes Spark functionality available in the Python interpreter. Spark is under vigorous development. Because the interface is evolving it's important to know what version you're working with. We'll be using version 2.4.1, which was released in March 2019.\n",
    "\n",
    "4. Sub-modules\n",
    "\n",
    "In addition to the main pyspark module, there are a few sub-modules which implement different aspects of the Spark interface. There are two versions of Spark Machine Learning: mllib, which uses an unstructured representation of data in RDDs and has been deprecated, and ml which is based on a structured, tabular representation of data in DataFrames. We'll be using the latter.\n",
    "\n",
    "5. Spark URL\n",
    "\n",
    "With the pyspark module loaded, you are able to connect to Spark. The next thing you need to do is tell Spark where the cluster is located. Here there are two options. You can either connect to a remote cluster, in which case you need to specify a Spark URL, which gives the network location of the cluster's master node. The URL is composed of an IP address or DNS name and a port number. The default port for Spark is 7077, but this must still be explicitly specified. When you're figuring out how Spark works, the infrastructure of a distributed cluster can get in the way. That's why it's useful to create a local cluster, where everything happens on a single computer. This is the setup that you're going to use throughout this course. For a local cluster, you need only specify \"local\" and, optionally, the number of cores to use. By default, a local cluster will run on a single core. Alternatively, you can give a specific number of cores or simply use the wildcard to choose all available cores.\n",
    "\n",
    "6. Creating a SparkSession\n",
    "\n",
    "You connect to Spark by creating a SparkSession object. The SparkSession class is found in the pyspark.sql sub-module. You specify the location of the cluster using the master() method. Optionally you can assign a name to the application using the appName() method. Finally you call the getOrCreate() method, which will either create a new session object or return an existing object. Once the session has been created you are able to interact with Spark. Finally, although it's possible for multiple SparkSessions to co-exist, it's good practice to stop the SparkSession when you're done.\n",
    "\n",
    "7. Let's connect to Spark!\n",
    "\n",
    "Great! Let's connect to Spark!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea457285-eed7-4fd5-a3a7-6c4825682f4f",
   "metadata": {},
   "source": [
    "### Location of Spark master\n",
    "Which of the following is not a valid way to specify the location of a Spark cluster?\n",
    "\n",
    "Answer the question\n",
    "\n",
    "- spark://13.59.151.161:7077\n",
    "\n",
    "- spark://ec2-18-188-22-23.us-east-2.compute.amazonaws.com:7077\n",
    "\n",
    "- **spark://18.188.22.23**\n",
    "\n",
    "- local\n",
    "\n",
    "- local[4]\n",
    "\n",
    "- local[*]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2555cbc8-4b07-40d5-bc41-2d5fc2301605",
   "metadata": {},
   "source": [
    "### Creating a SparkSession\n",
    "In this exercise, you'll spin up a local Spark cluster using all available cores. The cluster will be accessible via a `SparkSession` object.\n",
    "\n",
    "The `SparkSession` class has a builder attribute, which is an instance of the Builder class. The Builder class exposes three important methods that let you:\n",
    "\n",
    "- specify the location of the master node;\n",
    "- name the application (optional); and\n",
    "- retrieve an existing `SparkSession` or, if there is none, create a new one.\n",
    "- The `SparkSession` class has a version attribute that gives the version of Spark. Note: The version can also be accessed via the `__version__` attribute on the `pyspark` module.\n",
    "\n",
    "Find out more about `SparkSession` [here](https://spark.apache.org/docs/2.3.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession).\n",
    "\n",
    "Once you are finished with the cluster, it's a good idea to shut it down, which will free up its resources, making them available for other processes.\n",
    "\n",
    "**Note**:: You might find it useful to review the slides from the lessons in the Slides panel next to the *IPython Shell*.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import the `SparkSession` class from `pyspark.sql`.\n",
    "- Create a `SparkSession` object connected to a local cluster. Use all available cores. Name the application `'test'`.\n",
    "- Use the version attribute on the `SparkSession` object to retrieve the version of Spark running on the cluster. **Note**: The version might be different from the one that's used in the presentation (it gets updated from time to time).\n",
    "- Shut down the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "969ab500-007e-40f2-a4fb-646defff53be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.0\n"
     ]
    }
   ],
   "source": [
    "# Import the SparkSession class\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession object\n",
    "spark = SparkSession.builder \\\n",
    "                    .master('local[*]') \\\n",
    "                    .appName('test') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# What version of Spark?\n",
    "print(spark.version)\n",
    "\n",
    "# Terminate the cluster\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1790853-296d-4b44-9910-88aee24c2465",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "1. Loading Data\n",
    "\n",
    "In this lesson you'll look at how to read data into Spark.\n",
    "\n",
    "3. DataFrames: A refresher\n",
    "\n",
    "Spark represents tabular data using the DataFrame class. The data are captured as rows (or \"records\"), each of which is broken down into one or more columns (or \"fields\"). Every column has a name and a specific data type. Some selected methods and attributes of the DataFrame class are listed here. The count() method gives the number of rows. The show() method will display a subset of rows. The printSchema() method and the dtypes attribute give different views on column types. This is really scratching the surface of what's possible with a DataFrame. You can find out more by consulting the extensive documentation.\n",
    "\n",
    "4. CSV data for cars\n",
    "\n",
    "CSV is a common format for storing tabular data. For illustration we'll be using a CSV file with characteristics for a selection of motor vehicles. Each line in a CSV file is a new record and within each record, fields are separated by a delimiter character, which is normally a comma. The first line is an optional header record which gives column names.\n",
    "\n",
    "5. Reading data from CSV\n",
    "\n",
    "Our session object has a \"read\" attribute which, in turn, has a csv() method which reads data from a CSV file and returns a DataFrame. The csv() method has one mandatory argument, the path to the CSV file. There are a number of optional arguments. We'll take a quick look at some of the most important ones. The header argument specifies whether or not there is a header record. The sep argument gives the field separator, which is a comma by default. There are two arguments which pertain to column data types, schema and inferSchema. Finally, the nullValue argument gives the placeholder used to indicate missing data. Let's take a look at the data we've just loaded.\n",
    "\n",
    "6. Peek at the data\n",
    "\n",
    "Using the show() method we can take a look at a slice of the DataFrame. The csv() method has split the data into rows and columns and picked up the column names from the header record. Looks great, doesn't it? Unfortunately there's a small snag. Before we unravel that snag, it's important to note that the first value in the cylinder column is not a number. It's the string \"NA\" which indicates missing data.\n",
    "\n",
    "7. Check column types\n",
    "\n",
    "If you check the column data types then you'll find that they are all strings. That doesn't make sense since the last six columns are clearly numbers! However, this is the expected behavior: the csv() method treats all columns as strings by default. You need to do a little more work to get the correct column types. There are two ways that you can do this: infer the column types from the data or manually specify the types.\n",
    "\n",
    "8. Inferring column types from data\n",
    "\n",
    "It's possible to reasonably deduce the column types by setting the inferSchema argument to True. There is a price to pay though: Spark needs to make an extra pass over the data to figure out the column types before reading the data. If the data file is big then this will increase load time notably. Using this approach all of the column types are correctly identified except for cylinder. Why? The first value in this column is \"NA\", so Spark thinks that the column contains strings.\n",
    "\n",
    "9. Dealing with missing data\n",
    "\n",
    "Missing data in CSV files are normally represented by a placeholder like the \"NA\" string. You can use the nullValue argument to specify the placeholder. It's always a good idea to explicitly define the missing data placeholder. The nullValue argument is case sensitive, so it's important to provide it in exactly the same form as it appears in the data file.\n",
    "\n",
    "10. Specify column types\n",
    "\n",
    "If inferring column type is not successful then you have the option of specifying the type of each column in an explicit schema. This also makes it possible to choose alternative column names.\n",
    "\n",
    "11. Final cars data\n",
    "\n",
    "This is what the final cars data look like. Note that the missing value at the top of the cylinders column is indicated by the special null constant.\n",
    "\n",
    "12. Let's load some data!\n",
    "\n",
    "You're ready to use what you've learned to load data from CSV files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78a66096",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://AAsmny-lt-11105.itworx.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=test>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a924882e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://AAsmny-lt-11105.itworx.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x165da3c6e50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc).builder\\\n",
    ".master(\"local[2]\")\\\n",
    ".appName(\"MLSpark\")\\\n",
    ".getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99612b64-9d8e-4614-9c8b-2d5c180a1fb6",
   "metadata": {},
   "source": [
    "### Loading flights data\n",
    "In this exercise, you're going to load some airline flight data from a CSV file. To ensure that the exercise runs quickly these data have been trimmed down to only 50 000 records. You can get a larger dataset in the same format [here](https://assets.datacamp.com/production/repositories/3918/datasets/e1c1a03124fb2199743429e9b7927df18da3eacf/flights-larger.csv).\n",
    "\n",
    "Notes on CSV format:\n",
    "\n",
    "- fields are separated by a comma (this is the default separator) and\n",
    "- missing data are denoted by the string 'NA'.\n",
    "\n",
    "**Data dictionary**:\n",
    "\n",
    "- `mon` — month (integer between 1 and 12)\n",
    "- `dom` — day of month (integer between 1 and 31)\n",
    "- `dow` — day of week (integer; 1 = Monday and 7 = Sunday)\n",
    "- `carrier` — carrier (IATA code)\n",
    "- `flight` — flight number\n",
    "- `org` — origin airport (IATA code)\n",
    "- `mile` — distance (miles)\n",
    "- `depart` — departure time (decimal hour)\n",
    "- `duration` — expected duration (minutes)\n",
    "- `delay` — delay (minutes)\n",
    "\n",
    "`pyspark` has been imported for you and the session has been initialized.\n",
    "\n",
    "Note: The data have been aggressively down-sampled.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Read data from a CSV file called `'flights.csv'`. Assign data types to columns automatically. Deal with missing data.\n",
    "- How many records are in the data?\n",
    "- Take a look at the first five records.\n",
    "- What data types have been assigned to the columns? Do these look correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29a5010f-81b1-4d9f-81db-424d09f6e296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contain 50000 records.\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| null|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|\n",
      "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|\n",
      "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|\n",
      "|  4|  2|  5|     AA|   325|ORD| 258|  8.92|      65| null|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "[('mon', 'int'), ('dom', 'int'), ('dow', 'int'), ('carrier', 'string'), ('flight', 'int'), ('org', 'string'), ('mile', 'int'), ('depart', 'double'), ('duration', 'int'), ('delay', 'int')]\n"
     ]
    }
   ],
   "source": [
    "# Read data from CSV file\n",
    "flights = spark.read.csv('flights.csv',\n",
    "                         sep=',',\n",
    "                         header=True,\n",
    "                         inferSchema=True,\n",
    "                         nullValue='NA')\n",
    "\n",
    "# Get number of records\n",
    "print(\"The data contain %d records.\" % flights.count())\n",
    "\n",
    "# View the first five records\n",
    "flights.show(5)\n",
    "\n",
    "# Check column data types\n",
    "print(flights.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af462366-2564-4c6a-b9a2-0baa0f2feac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contain 275000 records.\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "| 10| 10|  1|     OO|  5836|ORD| 157|  8.18|      51|   27|\n",
      "|  1|  4|  1|     OO|  5866|ORD| 466|  15.5|     102| null|\n",
      "| 11| 22|  1|     OO|  6016|ORD| 738|  7.17|     127|  -19|\n",
      "|  2| 14|  5|     B6|   199|JFK|2248| 21.17|     365|   60|\n",
      "|  5| 25|  3|     WN|  1675|SJC| 386| 12.92|      85|   22|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "[('mon', 'int'), ('dom', 'int'), ('dow', 'int'), ('carrier', 'string'), ('flight', 'int'), ('org', 'string'), ('mile', 'int'), ('depart', 'double'), ('duration', 'int'), ('delay', 'int')]\n"
     ]
    }
   ],
   "source": [
    "# Read data from CSV file\n",
    "flights_full = spark.read.csv('flights-larger.csv',\n",
    "                         sep=',',\n",
    "                         header=True,\n",
    "                         inferSchema=True,\n",
    "                         nullValue='NA')\n",
    "\n",
    "# Get number of records\n",
    "print(\"The data contain %d records.\" % flights_full.count())\n",
    "\n",
    "# View the first five records\n",
    "flights_full.show(5)\n",
    "\n",
    "# Check column data types\n",
    "print(flights_full.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd05275-f505-4bf8-b86a-2d9315a8ffeb",
   "metadata": {},
   "source": [
    "### Loading SMS spam data\n",
    "You've seen that it's possible to infer data types directly from the data. Sometimes it's convenient to have direct control over the column types. You do this by defining an explicit schema.\n",
    "\n",
    "The file `sms.csv` contains a selection of SMS messages which have been classified as either `'spam'` or `'ham'`. These data have been adapted from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). There are a total of 5574 SMS, of which 747 have been labeled as spam.\n",
    "\n",
    "**Notes on CSV format**:\n",
    "\n",
    "- no header record and\n",
    "- fields are separated by a semicolon (this is not the default separator).\n",
    "\n",
    "**Data dictionary**:\n",
    "\n",
    "- `id` — record identifier\n",
    "- `text` — content of SMS message\n",
    "- `label` — spam or ham (integer; 0 = ham and 1 = spam)\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Specify the data schema, giving columns names (`\"id\"`, `\"text\"`, and `\"label\"`) and column types.\n",
    "- Read data from a delimited file called `\"sms.csv\"`.\n",
    "- Print the schema for the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eef7bb2-fb63-4d06-9f41-b8acc9e9ec4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Specify column names and types\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"label\", IntegerType())\n",
    "])\n",
    "\n",
    "# Load data from a delimited file\n",
    "sms = spark.read.csv('sms.csv', sep=';', header=False, schema=schema)\n",
    "\n",
    "# Print schema of DataFrame\n",
    "sms.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651dbc48-1f6d-4553-a549-428913584829",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159bd702-ca88-438a-8de9-11b1af62f280",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "1. Data Preparation\n",
    "\n",
    "In this lesson you are going to learn how to prepare data for building a Machine Learning model.\n",
    "\n",
    "2. Do you need all of those columns?\n",
    "\n",
    "You'll be working with the cars data again. This is what the data look like at present. There are columns for the maker and model, the origin (either USA or non-USA), the type, number of cylinders, engine size, weight, length, RPM and fuel consumption. The models that you'll be building will depend on the physical characteristics of the cars rather than the model names or manufacturers, so you'll remove the corresponding columns from the data.\n",
    "\n",
    "3. Dropping columns\n",
    "\n",
    "There are two approaches to doing this: either you can drop() the columns that you don't want or you can select() the fields which you do want to retain. Either way, the resulting data does not include those columns.\n",
    "\n",
    "4. Filtering out missing data\n",
    "\n",
    "Earlier you saw that there is a missing value in the cylinders column. Let's check to see how many other missing values there are. You'll use the filter() method and provide a logical predicate using SQL syntax which identifies NULL values. Then the count() method tells you how many records there are remaining. Just one. In this case it makes sense to simply remove the record with the missing value. There are a couple of ways that you could to do this. You could use the filter() method again with a different predicate. Or you could take a more aggressive approach and use the dropna() method to drop all records with missing values in any column. However, this should be done with care because it could result in the loss of a lot of otherwise useful data. You've now stripped down the data to what's needed to build a model.\n",
    "\n",
    "5. Mutating columns\n",
    "\n",
    "At present the weight and length columns are in units of pounds and inches respectively. You'll use the withColumn() method to create a new mass column in units of kilograms. The round() function is used to limit the precision of the result. You can also use the withColumn() method to replace the existing length column with values in meters. You now have mass and length in metric units.\n",
    "\n",
    "6. Indexing categorical data\n",
    "\n",
    "The type column consists of strings which represent six categories of vehicle type. You'll need to transform those strings into numbers. You do this using an instance of the StringIndexer class. In the constructor you provide the name of the string input column and a name for the new output column to be created. The indexer is first fit to the data, creating a StringIndexerModel. During the fitting process the distinct string values are identified and an index is assigned to each value. The model is then used to transform the data, creating a new column with the index values. By default the index values are assigned according to the descending relative frequency of each of the string values. Midsize is most common, so it gets an index of zero. Small is next most common, so its index is one. And so on. It's possible to choose different strategies for assigning index values by specifying the stringOrderType argument. Rather than using frequency of occurrence, strings can be ordered alphabetically. It's also possible to choose between ascending and descending order.\n",
    "\n",
    "7. Indexing country of origin\n",
    "\n",
    "You'll be building a classifier to predict whether or not a car was manufactured in the USA. So the origin column also needs to be converted from strings into numbers.\n",
    "\n",
    "8. Assembling columns\n",
    "\n",
    "The final step in preparing the cars data is to consolidate the various input columns into a single column. This is necessary because the Machine Learning algorithms in Spark operate on a single vector of predictors, although each element in that vector may consist of multiple values. To illustrate the process you'll start with just a pair of features, cylinders and size. First you create an instance of the VectorAssembler class, providing it with the names of the columns that you want to consolidate and the name of the new output column. The assembler is then used to transform the data. Taking a look at the relevant columns you see that the new \"features\" column consists of values from the cylinders and size columns consolidated into a vector. Ultimately you are going to assemble all of the predictors into a single column.\n",
    "\n",
    "9. Let's practice!\n",
    "\n",
    "Let's try out what we have learned on the SMS and flights data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f384d824-cfe0-4b48-97b3-6beebd12e9c5",
   "metadata": {},
   "source": [
    "### Removing columns and rows\n",
    "You previously loaded airline flight data from a CSV file. You're going to develop a model which will predict whether or not a given flight will be delayed.\n",
    "\n",
    "In this exercise, you need to trim those data down by:\n",
    "\n",
    "- removing an uninformative column and\n",
    "- removing rows that do not have information about whether or not a flight was delayed.\n",
    "\n",
    "The data are available as `flights`.\n",
    "\n",
    "**Note**:: You might find it helpful to revise the slides from the lessons in the Slides panel next to the IPython Shell.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Remove the `flight` column.\n",
    "- Find out how many records have missing values in the `delay` column.\n",
    "- Remove records with missing values in the `delay` column.\n",
    "- Remove records with missing values in any column and get the number of remaining rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53fcbbd8-5ae9-4023-ad89-a29ed61c087c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47022\n"
     ]
    }
   ],
   "source": [
    "# Remove the 'flight' column\n",
    "flights_drop_column = flights.drop('flight')\n",
    "\n",
    "# Number of records with missing 'delay' values\n",
    "flights_drop_column.filter('delay IS NULL').count()\n",
    "\n",
    "# Remove records with missing 'delay' values\n",
    "flights_valid_delay = flights_drop_column.filter('delay IS NOT NULL')\n",
    "\n",
    "# Remove records with missing values in any column and get the number of remaining rows\n",
    "flights_none_missing = flights_valid_delay.dropna()\n",
    "print(flights_none_missing.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa678017-0113-455f-a308-2ea3ad4bfd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2978\n"
     ]
    }
   ],
   "source": [
    "print(flights_drop_column.filter('delay IS NULL').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1238d6b-fa58-4ce3-addb-7cf2bd57f287",
   "metadata": {},
   "source": [
    "### Column manipulation\n",
    "The Federal Aviation Administration (FAA) considers a flight to be \"delayed\" when it arrives 15 minutes or more after its scheduled time.\n",
    "\n",
    "The next step of preparing the flight data has two parts:\n",
    "\n",
    "- convert the units of distance, replacing the `mile` column with a `km` column; and\n",
    "- create a Boolean column indicating whether or not a flight was delayed.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import a function which will allow you to round a number to a specific number of decimal places.\n",
    "- Derive a new `km` column from the `mile` column, rounding to zero decimal places. One mile is 1.60934 km.\n",
    "- Remove the `mile` column.\n",
    "- Create a label column with a value of `1` indicating the delay was 15 minutes or more and `0` otherwise. Think carefully about the logical condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ecd6d16-82d4-4059-896a-c53ffd3a8e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| null|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|\n",
      "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|\n",
      "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|\n",
      "|  4|  2|  5|     AA|   325|ORD| 258|  8.92|      65| null|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4d0a88b-3f65-499b-a427-63445da6a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_backup = flights\n",
    "flights = flights_none_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "666e39d3-9a57-4646-83fe-197db25f3762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the required function\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Convert 'mile' to 'km' and drop 'mile' column (1 mile is equivalent to 1.60934 km)\n",
    "flights_km = flights.withColumn('km', round(flights['mile'] * 1.60934, 0)) \\\n",
    "                    .drop('mile')\n",
    "\n",
    "# Create 'label' column indicating whether flight delayed (1) or not (0)\n",
    "flights_km = flights_km.withColumn('label', (flights_km.delay >= 15).cast('integer'))\n",
    "\n",
    "# Check first five records\n",
    "flights_km.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41db3910-17eb-4e53-ac69-23e23d1738f7",
   "metadata": {},
   "source": [
    "### Categorical columns\n",
    "In the flights data there are two columns, `carrier` and `org`, which hold categorical data. You need to transform those columns into indexed numerical values.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import the appropriate class and create an indexer object to transform the `carrier` column from a string to an numeric index.\n",
    "- Prepare the indexer object on the flight data.\n",
    "- Use the prepared indexer to create the numeric index column.\n",
    "- Repeat the process for the org column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b4be0b8-a81f-4fde-a455-98506f1c6ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_backup2 = flights\n",
    "flights = flights_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6adc4e9e-b8b6-4c92-84cb-2793e217c580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.20.1)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|        0.0|    0.0|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|        0.0|    1.0|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|        1.0|    0.0|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|        0.0|    1.0|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|        1.0|    0.0|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Create an indexer\n",
    "indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')\n",
    "\n",
    "# Indexer identifies categories in the data\n",
    "indexer_model = indexer.fit(flights)\n",
    "\n",
    "# Indexer creates a new column with numeric index values\n",
    "flights_indexed = indexer_model.transform(flights)\n",
    "\n",
    "# Repeat the process for the other categorical feature\n",
    "flights_indexed = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights_indexed).transform(flights_indexed)\n",
    "flights_indexed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0165f4cc-2b7b-4bf9-9daf-a2bfab761528",
   "metadata": {},
   "source": [
    "### Assembling columns\n",
    "The final stage of data preparation is to consolidate all of the predictor columns into a single column.\n",
    "\n",
    "An updated version of the `flights` data, which takes into account all of the changes from the previous few exercises, has the following predictor columns:\n",
    "\n",
    "`mon`, `dom` and `dow`\n",
    "- `carrier_idx` (indexed value from carrier)\n",
    "- `org_idx` (indexed value from org)\n",
    "- `km`\n",
    "- `depart`\n",
    "- `duration`\n",
    "\n",
    "Note: The `truncate=False` argument to the `show()` method prevents data being truncated in the output.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import the class which will assemble the predictors.\n",
    "- Create an assembler object that will allow you to merge the predictors columns into a single column.\n",
    "- Use the assembler to generate a new consolidated column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be7c60c7-7ac9-4118-89bc-7d436767e1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|        0.0|    0.0|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|        0.0|    1.0|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|        1.0|    0.0|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_backup3 = flights\n",
    "flights = flights_indexed\n",
    "flights.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e78676fb-0aca-4413-a5c3-27f4e12ba217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-----+\n",
      "|features                                 |delay|\n",
      "+-----------------------------------------+-----+\n",
      "|[0.0,22.0,2.0,0.0,0.0,509.0,16.33,82.0]  |30   |\n",
      "|[2.0,20.0,4.0,0.0,1.0,542.0,6.17,82.0]   |-8   |\n",
      "|[9.0,13.0,1.0,1.0,0.0,1989.0,10.33,195.0]|-5   |\n",
      "|[5.0,2.0,1.0,0.0,1.0,885.0,7.98,102.0]   |2    |\n",
      "|[7.0,2.0,6.0,1.0,0.0,1180.0,10.83,135.0] |54   |\n",
      "+-----------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary class\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=['mon', 'dom', 'dow',\n",
    "    'carrier_idx', 'org_idx', 'km', 'depart', 'duration'\n",
    "], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "flights_assembled = assembler.transform(flights)\n",
    "\n",
    "# Check the resulting column\n",
    "flights_assembled.select('features', 'delay').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585279d9-136f-4322-b4c5-1b1e731a101d",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "1. Decision Tree\n",
    "\n",
    "Your first Machine Learning model will be a Decision Tree. This is probably the most intuitive model, so it seems like a good place to start.\n",
    "\n",
    "2. Anatomy of a Decision Tree: Root node\n",
    "\n",
    "A Decision Tree is constructed using an algorithm called \"Recursive Partitioning\". Consider a hypothetical example in which you build a Decision Tree to divide data into two classes, green and blue. You start by putting all of the records into the root node. Suppose that there are more green records than blue, in which case this node will be labelled \"green\". Now from amongst the predictors in the data you need to choose the one that will result in the most informative split of the data into two groups. Ideally you want the groups to be as homogeneous (or \"pure\") as possible: one should be mostly green and the other should be mostly blue.\n",
    "\n",
    "3. Anatomy of a Decision Tree: First split\n",
    "\n",
    "Once you have identified the most informative predictor, you split the data into two sets, labeled \"green\" or \"blue\" according to the dominant class. And this is where the recursion kicks in: you then apply exactly the same procedure on each of the child nodes, selecting the most informative predictor and splitting again.\n",
    "\n",
    "4. Anatomy of a Decision Tree: Second split\n",
    "\n",
    "So, for example, the green node on the left could be split again into two groups.\n",
    "\n",
    "5. Anatomy of a Decision Tree: Third split\n",
    "\n",
    "And the resulting green node could once again be split. The depth of each branch of the tree need not be the same. There are a variety of stopping criteria which can cause splitting to stop along a branch. For example, if the number of records in a node falls below a threshold or the purity of a node is above a threshold, then you might stop splitting. Once you have built the Decision Tree you can use it to make predictions for new data by following the splits from the root node along to the tip of a branch. The label for the final node would then be the prediction for the new data.\n",
    "\n",
    "6. Classifying cars\n",
    "\n",
    "Let's make this more concrete by looking at the cars data. You've transformed the country of origin column into a numeric index called 'label', with zero corresponding to cars manufactured in the USA and one for everything else. The remaining columns have all been consolidated into a column called 'features'. You want to build a Decision Tree which will use \"features\" to predict \"label\".\n",
    "\n",
    "7. Split train/test\n",
    "\n",
    "An important aspect of building a Machine Learning model is being able to assess how well it works. In order to do this we use the randomSplit() method to randomly split our data into two sets, a training set and a testing set. The proportions may vary, but generally you're looking at something like an 80:20 split, which means that the training set ends up having around 4 times as many records as the testing set.\n",
    "\n",
    "8. Build a Decision Tree model\n",
    "\n",
    "Finally the moment has come, you're going to build a Decision Tree. You start by creating a DecisionTreeClassifier() object. The next step is to fit the model to the training data by calling the fit() method.\n",
    "\n",
    "9. Evaluating\n",
    "\n",
    "Now that you've trained the model you can assess how effective it is by making predictions on the test set and comparing the predictions to the known values. The transform() method adds new columns to the DataFrame. The prediction column gives the class assigned by the model. You can compare this directly to the known labels in the testing data. Although the model gets the first example wrong, it's correct for the following four examples. There's also a probability column which gives the probabilities assigned to each of the outcome classes. For the first example, the model predicts that the outcome is 0 with probability 96%.\n",
    "\n",
    "10. Confusion matrix\n",
    "\n",
    "A good way to understand the performance of a model is to create a confusion matrix which gives a breakdown of the model predictions versus the known labels. The confusion matrix consists of four counts which are labelled as follows: - \"positive\" indicates a prediction of 1, while - \"negative\" indicates a prediction of 0 and - \"true\" corresponds to a correct prediction, while - \"false\" designates an incorrect prediction. In this case the true positives and true negatives dominate but the model still makes a number of incorrect predictions. These counts can be used to calculate the accuracy, which is the proportion of correct predictions. For our model the accuracy is 74%.\n",
    "\n",
    "11. Let's build Decision Trees!\n",
    "\n",
    "So, now that you know how to build a Decision Tree model with Spark, you can try that out on the flight data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54149c54-3b22-496c-a208-0a09033fdd7e",
   "metadata": {},
   "source": [
    "### Train/test split\n",
    "To objectively assess a Machine Learning model you need to be able to test it on an independent set of data. You can't use the same data that you used to train the model: of course the model will perform (relatively) well on those data!\n",
    "\n",
    "You will split the data into two components:\n",
    "\n",
    "- training data (used to train the model) and\n",
    "- testing data (used to test the model).\n",
    "\n",
    "**Note**: From here on you'll be working with a smaller subset of the flights data, which just makes the exercises run more quickly.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Randomly split the `flights` data into two sets with 80:20 proportions. For repeatability set a random number seed of 43 for the split.\n",
    "- Check that the training data has roughly 80% of the records from the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5507345-8cfe-4f5e-8aef-b61c5216be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_backup3 = flights\n",
    "flights = flights_assembled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b34f9116-8911-4b3f-9fb2-1f543101af93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8025392369529156\n"
     ]
    }
   ],
   "source": [
    "# Split into training and testing sets in a 80:20 ratio\n",
    "flights_train, flights_test = flights.randomSplit([0.8,0.2], seed = 43)\n",
    "\n",
    "# Check that training set has around 80% of records\n",
    "training_ratio = flights_train.count() / flights.count()\n",
    "print(training_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca0a2c7-f813-4a6d-b78c-5505d1f50b28",
   "metadata": {},
   "source": [
    "### Build a Decision Tree\n",
    "Now that you've split the flights data into training and testing sets, you can use the training set to fit a Decision Tree model.\n",
    "\n",
    "The data are available as `flights_train` and `flights_test`.\n",
    "\n",
    "**NOTE**: It will take a few seconds for the model to train… please be patient!\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import the class for creating a Decision Tree classifier.\n",
    "- Create a classifier object and fit it to the training data.\n",
    "- Make predictions for the testing data and take a look at the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c58f6da-74a0-42a9-b841-98c7568837b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------------------------------------+\n",
      "|label|prediction|probability                            |\n",
      "+-----+----------+---------------------------------------+\n",
      "|1    |0.0       |[0.5319789315274642,0.4680210684725357]|\n",
      "|1    |0.0       |[0.5319789315274642,0.4680210684725357]|\n",
      "|0    |1.0       |[0.3554263565891473,0.6445736434108527]|\n",
      "|1    |1.0       |[0.3554263565891473,0.6445736434108527]|\n",
      "|1    |1.0       |[0.3554263565891473,0.6445736434108527]|\n",
      "+-----+----------+---------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the Decision Tree Classifier class\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create a classifier object and fit to the training data\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_model = tree.fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "prediction = tree_model.transform(flights_test)\n",
    "prediction.select('label', 'prediction', 'probability').show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a900e668-85f5-410a-9379-f50df505c17e",
   "metadata": {},
   "source": [
    "### Evaluate the Decision Tree\n",
    "You can assess the quality of your model by evaluating how well it performs on the testing data. Because the model was not trained on these data, this represents an objective assessment of the model.\n",
    "\n",
    "A confusion matrix gives a useful breakdown of predictions versus known values. It has four cells which represent the counts of:\n",
    "\n",
    "- True Negatives (TN) — model predicts negative outcome & known outcome is negative\n",
    "- True Positives (TP) — model predicts positive outcome & known outcome is positive\n",
    "- False Negatives (FN) — model predicts negative outcome but known outcome is positive\n",
    "- False Positives (FP) — model predicts positive outcome but known outcome is negative.\n",
    "These counts (`TN`, `TP`, `FN` and `FP`) should sum to the number of records in the testing data, which is only a subset of the flights data. You can compare to the number of records in the tests data, which is `flights_test.count()`.\n",
    "\n",
    "**Note**: These predictions are made on the testing data, so the counts are smaller than they would have been for predictions on the training data.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Create a confusion matrix by counting the combinations of label and prediction. Display the result.\n",
    "- Count the number of True Negatives, True Positives, False Negatives and False Positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76a4e06d-d796-425e-ab09-282316b09aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 1231|\n",
      "|    0|       0.0| 2404|\n",
      "|    1|       1.0| 3532|\n",
      "|    0|       1.0| 2118|\n",
      "+-----+----------+-----+\n",
      "\n",
      "0.6393107162089392\n"
     ]
    }
   ],
   "source": [
    "# Create a confusion matrix\n",
    "prediction.groupBy('label', 'prediction').count().show()\n",
    "\n",
    "# Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label <> prediction').count()\n",
    "FP = prediction.filter('prediction = 1 AND label <> prediction').count()\n",
    "\n",
    "# Accuracy measures the proportion of correct predictions\n",
    "accuracy = (TP + TN)/ flights_test.count()\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9a925e6-a50b-4d8e-b0cb-5b89c8a81c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3532 2404 2118 1231\n"
     ]
    }
   ],
   "source": [
    "print(TP, TN, FP, FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f08bef-ff8a-40a8-809d-792a483519d5",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "1. Logistic Regression\n",
    "\n",
    "You've learned to build a Decision Tree. But it's good to have options. Logistic Regression is another commonly used classification model.\n",
    "\n",
    "2. Logistic Curve\n",
    "\n",
    "It uses a logistic function to model a binary target, where the target states are usually denoted by 1 and 0 or TRUE and FALSE. The maths of the model are outside the scope of this course, but this is what the logistic function looks like. For a Logistic Regression model the x-axis is a linear combination of predictor variables and the y-axis is the output of the model. Since the value of the logistic function is a number between zero and one, it's often thought of as a probability. In order to translate this number into one or other of the target states it's compared to a threshold, which is normally set at one half.\n",
    "\n",
    "3. Logistic Curve\n",
    "\n",
    "If the number is above the threshold then the predicted state is one.\n",
    "\n",
    "4. Logistic Curve\n",
    "\n",
    "Conversely, if it's below the threshold then the predicted state is zero. The model derives coefficients for each of the numerical predictors. Those coefficients might...\n",
    "\n",
    "5. Logistic Curve\n",
    "\n",
    "shift the curve to the right...\n",
    "\n",
    "6. Logistic Curve\n",
    "\n",
    "or to the left. They might make the transition between states...\n",
    "\n",
    "7. Logistic Curve\n",
    "\n",
    "more gradual...\n",
    "\n",
    "8. Logistic Curve\n",
    "\n",
    "or more rapid. These characteristics are all extracted from the training data and will vary from one set of data to another.\n",
    "\n",
    "9. Cars revisited\n",
    "\n",
    "Let's make this more concrete by returning to the cars data. You'll focus on the numerical predictors for the moment and return to categorical predictors later on. As before you prepare the data by consolidating the predictors into a single column and then randomly splitting the data into training and testing sets.\n",
    "\n",
    "10. Build a Logistic Regression model\n",
    "\n",
    "To build a Logistic Regression model you first need to import the associated class and then create a classifier object. This is then fit to the training data using the fit() method.\n",
    "\n",
    "11. Predictions\n",
    "\n",
    "With a trained model you are able to make predictions on the testing data. As you saw with the Decision Tree, the transform() method adds the prediction and probability columns. The probability column gives the predicted probability of each class, while the prediction column reflects the predicted label, which is derived from the probabilities by applying the threshold mentioned earlier.\n",
    "\n",
    "12. Precision and recall\n",
    "\n",
    "You can assess the quality of the predictions by forming a confusion matrix. The quantities in the cells of the matrix can then be used to form some informative ratios. Recall that a positive prediction indicates that a car is manufactured outside of the USA and that predictions are considered to be true or false depending on whether they are correct or not. Precision is the proportion of positive predictions which are correct. For your model, two thirds of predictions for cars manufactured outside of the USA are correct. Recall is the proportion of positive targets which are correctly predicted. Your model also identifies 80% of cars which are actually manufactured outside of the USA. Bear in mind that these metrics are based on a relatively small testing set.\n",
    "\n",
    "13. Weighted metrics\n",
    "\n",
    "Another way of looking at these ratios is to weight them across the positive and negative predictions. You can do this by creating an evaluator object and then calling the evaluate() method. This method accepts an argument which specifies the required metric. It's possible to request the weighted precision and recall as well as the overall accuracy. It's also possible to get the F1 metric, the harmonic mean of precision and recall, which is generally more robust than the accuracy. All of these metrics have assumed a threshold of one half. What happens if you vary that threshold?\n",
    "\n",
    "14. ROC and AUC\n",
    "\n",
    "A threshold is used to decide whether the number returned by the Logistic Regression model translates into either the positive or the negative class. By default that threshold is set at a half. However, this is not the only choice. Choosing a larger or smaller value for the threshold will affect the performance of the model. The ROC curve plots the true positive rate versus the false positive rate as the threshold increases from zero (top right) to one (bottom left). The AUC summarizes the ROC curve in a single number. It's literally the area under the ROC curve. AUC indicates how well a model performs across all values of the threshold. An ideal model, that performs perfectly regardless of the threshold, would have AUC of 1. In an exercise we'll see how to use another evaluator to calculate the AUC.\n",
    "\n",
    "15. Let's do Logistic Regression!\n",
    "\n",
    "You now know how to build a Logistic Regression model and assess the performance of that model using various metrics. Let's give this a try!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa59d1f0-19ad-45b0-b130-d7f195253d94",
   "metadata": {},
   "source": [
    "### Build a Logistic Regression model\n",
    "You've already built a Decision Tree model using the flights data. Now you're going to create a Logistic Regression model on the same data.\n",
    "\n",
    "The objective is to predict whether a flight is likely to be delayed by at least 15 minutes (label `1`) or not (label `0`).\n",
    "\n",
    "Although you have a variety of predictors at your disposal, you'll only use the `mon`, `depart` and `duration` columns for the moment. These are numerical features which can immediately be used for a Logistic Regression model. You'll need to do a little more work before you can include categorical features. Stay tuned!\n",
    "\n",
    "The data have been split into training and testing sets and are available as `flights_train` and `flights_test`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import the class for creating a Logistic Regression classifier.\n",
    "- Create a classifier object and train it on the training data.\n",
    "- Make predictions for the testing data and create a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e15ce1ee-170a-4578-8e55-26fd49312d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|        0.0|    0.0|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|        0.0|    1.0|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|        1.0|    0.0|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|        0.0|    1.0|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|        1.0|    0.0|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights = flights_backup3\n",
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce559d53-e334-49f4-8496-2e08dfdf0cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_backup4 = flights\n",
    "\n",
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=['mon','depart', 'duration'], outputCol='features')\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "flights_train, flights_test = assembler.transform(flights.select('mon','depart', 'duration','delay').\\\n",
    "withColumn('label' , when(flights.delay > 15 , 1).otherwise(0)).\\\n",
    "drop('delay')).\\\n",
    "randomSplit([0.8,0.2], seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc8b6f98-7a2d-4b3d-afd0-ca95362c8bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+-----+----------------+\n",
      "|mon|depart|duration|label|        features|\n",
      "+---+------+--------+-----+----------------+\n",
      "|  0|  0.25|     308|    0|[0.0,0.25,308.0]|\n",
      "|  0|  0.25|     308|    0|[0.0,0.25,308.0]|\n",
      "|  0|  0.25|     308|    0|[0.0,0.25,308.0]|\n",
      "|  0|  0.25|     308|    1|[0.0,0.25,308.0]|\n",
      "|  0|  0.25|     308|    1|[0.0,0.25,308.0]|\n",
      "+---+------+--------+-----+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de03455a-0fbe-4292-aea5-5831e08d0299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 1830|\n",
      "|    0|       0.0| 2670|\n",
      "|    1|       1.0| 2931|\n",
      "|    0|       1.0| 1990|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the logistic regression class\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a classifier object and train on training data\n",
    "logistic = LogisticRegression().fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and show confusion matrix\n",
    "prediction = logistic.transform(flights_test)\n",
    "prediction.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfebe983-46f2-4912-9ec8-d8e4d70d7577",
   "metadata": {},
   "source": [
    "### Evaluate the Logistic Regression model\n",
    "Accuracy is generally not a very reliable metric because it can be biased by the most common target class.\n",
    "\n",
    "There are two other useful metrics:\n",
    "\n",
    "- *precision* and\n",
    "- *recall*.\n",
    "Check the slides for this lesson to get the relevant expressions.\n",
    "\n",
    "Precision is the proportion of positive predictions which are correct. For all flights which are predicted to be delayed, what proportion is actually delayed?\n",
    "\n",
    "Recall is the proportion of positives outcomes which are correctly predicted. For all delayed flights, what proportion is correctly predicted by the model?\n",
    "\n",
    "The precision and recall are generally formulated in terms of the positive target class. But it's also possible to calculate weighted versions of these metrics which look at both target classes.\n",
    "\n",
    "The components of the confusion matrix are available as `TN`, `TP`, `FN` and `FP`, as well as the object `prediction`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Find the precision and recall.\n",
    "- Create a multi-class evaluator and evaluate weighted precision.\n",
    "- Create a binary evaluator and evaluate `AUC` using the `\"areaUnderROC\"` metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75a7821b-7337-495e-99bb-79bf3777f4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2670 2931 1830 1990\n"
     ]
    }
   ],
   "source": [
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label <> prediction').count()\n",
    "FP = prediction.filter('prediction = 1 AND label <> prediction').count()\n",
    "print(TN,TP,FN,FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7db30fda-107a-481e-ad8d-927dbade669a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.60\n",
      "recall    = 0.62\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "print('precision = {:.2f}\\nrecall    = {:.2f}'.format(precision, recall))\n",
    "\n",
    "# Find weighted precision\n",
    "multi_evaluator = MulticlassClassificationEvaluator()\n",
    "weighted_precision = multi_evaluator.evaluate(prediction, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "\n",
    "# Find AUC\n",
    "binary_evaluator = BinaryClassificationEvaluator()\n",
    "auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName: 'areaUnderROC'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c32c8fb-d6d1-427e-8fa1-9855e8874ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6271774963423306"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192fbeb7-648f-433c-8c1a-9b0839e83d6a",
   "metadata": {},
   "source": [
    "## Turning Text into Tables\n",
    "1. Turning Text into Tables\n",
    "\n",
    "It's said that 80% of Machine Learning is data preparation. As we'll see in this lesson, this is particularly true for text data. Before you can use Machine Learning algorithms you need to take unstructured text data and create structure, ultimately transforming the data into a table.\n",
    "\n",
    "2. One record per document\n",
    "\n",
    "We start with a collection of documents. These documents might be anything from a short snippet of text, like an SMS or email, to a lengthy report or book. Each document will become a record in the table.\n",
    "\n",
    "3. One document, many columns\n",
    "\n",
    "The text in each document will be mapped to columns in the table. First the text is split into words or tokens. You then remove short or common words that do not convey too much information. The table will then indicate the number of times that each of the remaining words occurred in the text. This table is also known as a \"term-document matrix\". There are some nuances to the process, but that's the central idea.\n",
    "\n",
    "4. A selection of children's books\n",
    "\n",
    "Suppose that your documents are the names of children's books. The raw data might look like this. Your job will be to transform these data into a table with one row per document and a column for each of the words.\n",
    "\n",
    "5. Removing punctuation\n",
    "\n",
    "You're interested in words, not punctuation. You'll use regular expressions (or REGEX), a mini-language for pattern matching, to remove the punctuation symbols. Regular expressions is another big topic and outside of the scope of this course, but basically you are giving a list of symbols or text pattern to match. The hyphen is escaped by the backslashes because it has another meaning in the context of regular expressions. By escaping it you tell Spark to interpret the hyphen literally. You need to specify a column name, books.text, a pattern to be matched (stored in the variable REGEX), and the replacement text, which is simply a space. You now have some double spaces but you can use REGEX to clean those up too.\n",
    "\n",
    "6. Text to tokens\n",
    "\n",
    "Next you split the text into words or tokens. You create a tokenizer object, giving it the name of the input column containing the text and the output column which will contain the tokens. The tokenizer is then applied to the text using the transform() method. In the results you see a new column in which each document has been transformed into a list of words. As a side effect the words have all been reduced to lower case.\n",
    "\n",
    "7. What are stop words?\n",
    "\n",
    "Some words occur frequently in all of the documents. These common or \"stop\" words convey very little information, so you will also remove them using an instance of the StopWordsRemover class. This contains a list of stop words which can be customized if necessary.\n",
    "\n",
    "8. Removing stop words\n",
    "\n",
    "Since you didn't give the input and output column names earlier, you specify them now and then apply the transform method. You could also have given these names when you created the remover.\n",
    "\n",
    "9. Feature hashing\n",
    "\n",
    "Your documents might contain a large variety of words, so in principle our table could end up with an enormous number of columns, many of which would be only sparsely populated. It would also be handy to convert the words into numbers. Enter the hashing trick, which in simple terms converts words into numbers. You create an instance of the HashingTF class, providing the names of the input and output columns. You also give the number of features, which is effectively the largest number that will be produced by the hashing trick. This needs to be sufficiently big to capture the diversity in the words. The output in the hash column is presented in sparse format, which we will talk about more later on. For the moment though it's enough to note that there are two lists. The first list contains the hashed values and the second list indicates how many times each of those values occurs. For example, in the first document the word \"long\" has a hash of 8 and occurs twice. Similarly, the word \"five\" has a hash of 6 and occurs once in each of the last two documents.\n",
    "\n",
    "10. Dealing with common words\n",
    "\n",
    "The final step is to account for some words occurring frequently across many documents. If a word appears in many documents then it's probably going to be less useful for building a classifier. We want to weight the number of counts for a word in a particular document against how frequently that word occurs across all documents. To do this you reduce the effective count for more common words, giving what is known as the \"inverse document frequency\". Inverse document frequency is generated by the IDF class, which is first fit to the hashed data and then used to generate weighted counts. The word \"five\", for example, occurs in multiple documents, so its effective frequency is reduced. Conversely, the word \"long\" only occurs in one document, so its effective frequency is increased.\n",
    "\n",
    "11. Text ready for Machine Learning!\n",
    "\n",
    "The inverse document frequencies are precisely what we need for building a Machine Learning model. Let's do that with the SMS data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446529c-4ea3-4b39-9449-4039c9f02131",
   "metadata": {},
   "source": [
    "### Punctuation, numbers and tokens\n",
    "At the end of the previous chapter you loaded a dataset of SMS messages which had been labeled as either `\"spam\"` (label `1`) or `\"ham\"` (label `0`). You're now going to use those data to build a classifier model.\n",
    "\n",
    "But first you'll need to prepare the SMS messages as follows:\n",
    "\n",
    "- remove punctuation and numbers\n",
    "- tokenize (split into individual words)\n",
    "- remove stop words\n",
    "- apply the hashing trick\n",
    "- convert to `TF-IDF` representation.\n",
    "In this exercise you'll remove punctuation and numbers, then tokenize the messages.\n",
    "\n",
    "The SMS data are available as `sms`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import the function to replace regular expressions and the feature to tokenize.\n",
    "- Replace all punctuation characters from the text column with a space. Do the same for all numbers in the text column.\n",
    "- Split the text column into tokens. Name the output column words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "def5eaf5-8af6-4c69-9313-fd98ed6e4787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------+-----+\n",
      "| id|                                              text|label|\n",
      "+---+--------------------------------------------------+-----+\n",
      "|  1|                 Sorry, I'll call later in meeting|    0|\n",
      "|  2|                    Dont worry. I guess he's busy.|    0|\n",
      "|  3|                 Call FREEPHONE 0800 542 0578 now!|    1|\n",
      "|  4|       Win a 1000 cash prize or a prize worth 5000|    1|\n",
      "|  5|Go until jurong point, crazy.. Available only i...|    0|\n",
      "+---+--------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sms.show(5,truncate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76e9b847-05ff-47ad-99ba-063b83be4053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------+-----+------------------------------------------+\n",
      "|id |text                              |label|words                                     |\n",
      "+---+----------------------------------+-----+------------------------------------------+\n",
      "|1  |Sorry I'll call later in meeting  |0    |[sorry, i'll, call, later, in, meeting]   |\n",
      "|2  |Dont worry I guess he's busy      |0    |[dont, worry, i, guess, he's, busy]       |\n",
      "|3  |Call FREEPHONE now                |1    |[call, freephone, now]                    |\n",
      "|4  |Win a cash prize or a prize worth |1    |[win, a, cash, prize, or, a, prize, worth]|\n",
      "+---+----------------------------------+-----+------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary functions\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# Remove punctuation (REGEX provided) and numbers\n",
    "wrangled = sms.withColumn('text', regexp_replace(sms.text, '[_():;,.!?\\\\-]', ' '))\n",
    "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, '[0-9]', ' '))\n",
    "\n",
    "# Merge multiple spaces\n",
    "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, ' +', ' '))\n",
    "\n",
    "# Split the text into words\n",
    "wrangled = Tokenizer(inputCol='text', outputCol='words').transform(wrangled)\n",
    "\n",
    "wrangled.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34f8912-45fb-4a8f-a88d-8ac6b5f9d5af",
   "metadata": {},
   "source": [
    "### Stop words and hashing\n",
    "The next steps will be to remove stop words and then apply the hashing trick, converting the results into a TF-IDF.\n",
    "\n",
    "A quick reminder about these concepts:\n",
    "\n",
    "- The hashing trick provides a fast and space-efficient way to map a very large (possibly infinite) set of items (in this case, all words contained in the SMS messages) onto a smaller, finite number of values.\n",
    "- The TF-IDF matrix reflects how important a word is to each document. It takes into account both the frequency of the word within each document but also the frequency of the word across all of the documents in the collection.\n",
    "The tokenized SMS data are stored in `sms` in a column named `words`. You've cleaned up the handling of spaces in the data so that the tokenized text is neater.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import the `StopWordsRemover`, `HashingTF` and `IDF` classes.\n",
    "- Create a `StopWordsRemover` object (input column `words`, output column `terms`). Apply to `sms`.\n",
    "- Create a `HashingTF` object (input results from previous step, output column `hash`). Apply to `wrangled`.\n",
    "- Create an `IDF` object (input results from previous step, output column `features`). Apply to `wrangled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3334947f-a55c-46a5-b5dc-3b52ff65d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_bsckup = sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "023e1978-2c29-4899-b012-2d2f6d8124cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms = wrangled.select('id', 'words','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "619abfcf-525d-45a8-bc03-81237c34fd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|terms                           |features                                                                                            |\n",
      "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|[sorry, call, later, meeting]   |(1024,[138,384,577,996],[2.273418200008753,3.6288353225642043,3.5890949939146903,4.104259019279279])|\n",
      "|[dont, worry, guess, busy]      |(1024,[215,233,276,329],[3.9913186080986836,3.3790235241678332,4.734227298217693,4.58299632849377]) |\n",
      "|[call, freephone]               |(1024,[133,138],[5.367951058306837,2.273418200008753])                                              |\n",
      "|[win, cash, prize, prize, worth]|(1024,[31,47,62,389],[3.6632029660684124,4.754846585420428,4.072170704727778,7.064594791043114])    |\n",
      "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "# Remove stop words.\n",
    "wrangled = StopWordsRemover(inputCol='words', outputCol='terms')\\\n",
    "      .transform(sms)\n",
    "\n",
    "# Apply the hashing trick\n",
    "wrangled = HashingTF(inputCol = 'terms', outputCol = 'hash', numFeatures=1024)\\\n",
    "      .transform(wrangled)\n",
    "\n",
    "# Convert hashed symbols to TF-IDF\n",
    "tf_idf = IDF(inputCol = 'hash', outputCol = 'features')\\\n",
    "      .fit(wrangled).transform(wrangled)\n",
    "      \n",
    "tf_idf.select('terms', 'features').show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd13514-6742-45ed-a4b0-475a74172a07",
   "metadata": {},
   "source": [
    "### Training a spam classifier\n",
    "The SMS data have now been prepared for building a classifier. Specifically, this is what you have done:\n",
    "\n",
    "- removed numbers and punctuation\n",
    "- split the messages into words (or \"tokens\")\n",
    "- removed stop words\n",
    "- applied the hashing trick and\n",
    "- converted to a TF-IDF representation.\n",
    "Next you'll need to split the TF-IDF data into training and testing sets. Then you'll use the training data to fit a Logistic Regression model and finally evaluate the performance of that model on the testing data.\n",
    "\n",
    "The data are stored in `sms` and `LogisticRegression` has been imported for you.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Split the data into training and testing sets in a 4:1 ratio. Set the random number seed to `13` to ensure repeatability.\n",
    "- Create a `LogisticRegression` object and fit it to the training data.\n",
    "- Generate predictions on the testing data.\n",
    "- Use the predictions to form a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c09c33aa-5ad2-49aa-98c5-215abf56e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_backup2 = sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad8ba7f1-9685-437a-91d0-65a14569242e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "| id|               words|label|               terms|                hash|            features|\n",
      "+---+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|  1|[sorry, i'll, cal...|    0|[sorry, call, lat...|(1024,[138,384,57...|(1024,[138,384,57...|\n",
      "|  2|[dont, worry, i, ...|    0|[dont, worry, gue...|(1024,[215,233,27...|(1024,[215,233,27...|\n",
      "|  3|[call, freephone,...|    1|   [call, freephone]|(1024,[133,138],[...|(1024,[133,138],[...|\n",
      "|  4|[win, a, cash, pr...|    1|[win, cash, prize...|(1024,[31,47,62,3...|(1024,[31,47,62,3...|\n",
      "|  5|[go, until, juron...|    0|[go, jurong, poin...|(1024,[12,171,191...|(1024,[12,171,191...|\n",
      "+---+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sms = tf_idf\n",
    "sms.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d59f431e-d700-446e-932a-087c1fb87ec2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|   41|\n",
      "|    0|       0.0|  948|\n",
      "|    1|       1.0|  105|\n",
      "|    0|       1.0|    2|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "sms_train, sms_test = sms.randomSplit([0.8,0.2], seed = 13)\n",
    "\n",
    "# Fit a Logistic Regression model to the training data\n",
    "logistic = LogisticRegression(regParam=0.2).fit(sms_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "prediction = logistic.transform(sms_test)\n",
    "\n",
    "# Create a confusion matrix, comparing predictions to known labels\n",
    "prediction.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dba77d-0128-4a94-a059-390e421193af",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b89c8f-e5fa-454d-89bd-3492178ce247",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "1. One-Hot Encoding\n",
    "\n",
    "In the last chapter you saw how to use categorical variables in a model by simply converting them to indexed numerical values. In general this is not sufficient for a regression model. Let's see why.\n",
    "\n",
    "2. The problem with indexed values\n",
    "\n",
    "In the cars data the type column is categorical, with six levels: 'Midsize', 'Small', 'Compact', 'Sporty', 'Large' and 'Van'. Here you can see the number of times that each of those levels occurrs in the data. You used a string indexer to assign a numerical index to each level. However, there's a problem with the index: the numbers don't have any objective meaning. The index for 'Sporty' is 3. Does it make sense to do arithmetic on that index? No. For example, it wouldn't be meaningful to add the index for 'Sporty' to the index for 'Compact'. Nor would it be valid to compare those indexes and say that 'Sporty' is larger or smaller than 'Compact'. However, a regression model works by doing precisely this: arithmetic on predictor variables. You need to convert the index values into a format in which you can perform meaningful mathematical operations.\n",
    "\n",
    "3. Dummy variables\n",
    "\n",
    "The first step is to create a column for each of the levels. Effectively you then place a check in the column corresponding to the value in each row. So, for example, a record with a type of 'Sporty' would have a check in the 'Sporty' column. These new columns are known as 'dummy variables'.\n",
    "\n",
    "4. Dummy variables: binary encoding\n",
    "\n",
    "However, rather than having checks in the dummy variable columns it makes more sense to use binary values, where a one indicates the presence of the corresponding level. It might occur to you that the volume of data has exploded. You've gone from a single column of categorical values to six binary encoded dummy variables. If there were more levels then you'd have even more columns. This could get out of hand. However, the majority of the cells in the new columns contain zeros. The non-zero values, which actually encode the information, are relatively infrequent. This effect becomes even more pronounced if there are more levels. You can exploit this by converting the data into a sparse format.\n",
    "\n",
    "5. Dummy variables: sparse representation\n",
    "02:37 - 02:49\n",
    "Rather than recording the individual values, the sparse representation simply records the column numbers and value for the non-zero values.\n",
    "\n",
    "6. Dummy variables: redundant column\n",
    "\n",
    "You can take this one step further. Since the categorical levels are mutually exclusive you can drop one of the columns. If type is not 'Midsize', 'Small', 'Compact', 'Sporty' or 'Large' then it must be 'Van'. The process of creating dummy variables is called 'One-Hot Encoding' because only one of the columns created is ever active or 'hot'. Let's see how this is done in Spark.\n",
    "\n",
    "7. One-hot encoding\n",
    "\n",
    "As you might expect, there's a class for doing one-hot encoding. Import the OneHotEncoder class from the feature sub-module. When instantiating the class you need to specify the names of the input and output columns. For car type the input column is the index we defined earlier. Choose 'type_dummy' as the output column name. Note that these arguments are given as lists, so it's possible to specify multiple columns if necessary. Next fit the encoder to the data. Check how many category levels have been identified: six as expected.\n",
    "\n",
    "8. One-hot encoding\n",
    "\n",
    "Now that the encoder is set up it can be applied to the data by calling the transform() method. Let's take a look at the results. There's now a type_dummy column which captures the dummy variables. As mentioned earlier, the final level is treated differently. No column is assigned to type Van because if a vehicle isn't one of the other types then it must be a Van. To have a separate dummy variable for Van would be redundant. The sparse format used to represent dummy variables looks a little complicated. Let's take a moment to dig into dense versus sparse formats.\n",
    "\n",
    "9. Dense versus sparse\n",
    "\n",
    "Suppose that you want to store a vector which consists mostly of zeros. You could store it as a dense vector, in which each of the elements of the vector is stored explicitly. This is wasteful though because most of those elements are zeros. A sparse representation is a much better alternative. To create a sparse vector you need to specify the size of the vector (in this case, eight), the positions which are non-zero (in this case, positions zero and five, noting that we start counting at zero) and the values for each of those positions, one and seven. Sparse representation is essential for effective one-hot encoding on large data sets.\n",
    "\n",
    "10. One-Hot Encode categoricals\n",
    "\n",
    "Let's try out one-hot encoding on the flights data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fcf5c6-a94e-4acf-aecd-c00816ea818a",
   "metadata": {},
   "source": [
    "### Encoding flight origin\n",
    "The org column in the flights data is a categorical variable giving the airport from which a flight departs.\n",
    "\n",
    "- ORD — O'Hare International Airport (Chicago)\n",
    "- SFO — San Francisco International Airport\n",
    "- JFK — John F Kennedy International Airport (New York)\n",
    "- LGA — La Guardia Airport (New York)\n",
    "- SMF — Sacramento\n",
    "- SJC — San Jose\n",
    "- OGG — Kahului (Hawaii)\n",
    "Obviously this is only a small subset of airports. Nevertheless, since this is a categorical variable, it needs to be one-hot encoded before it can be used in a regression model.\n",
    "\n",
    "The data are in a variable called `flights`. You have already used a string indexer to create a column of indexed values corresponding to the strings in `org`.\n",
    "\n",
    "You might find it useful to revise the slides from the lessons in the Slides panel next to the IPython Shell.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import the one-hot encoder class.\n",
    "- Create a one-hot encoder instance, naming the input column `org_idx` and the output column `org_dummy`.\n",
    "- Apply the one-hot encoder to the flights data.\n",
    "- Generate a summary of the mapping from categorical values to binary encoded dummy variables. Include only unique values and order by `org_idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb64ca39-06c7-42a6-bb32-ba27fa5dca52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|        0.0|    0.0|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|        0.0|    1.0|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|        1.0|    0.0|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|        0.0|    1.0|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|        1.0|    0.0|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_backup5 = flights\n",
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca49fa02-fb1a-425a-accb-f8ac47791424",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Steps seems to be done before\n",
    "#flights_indexed = StringIndexer(inputCol = 'org', outputCol = 'org_idx').fit(flights).transform(flights)\n",
    "#flights_indexed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e94c9b49-b120-4e34-bf44-505e0e264328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------+\n",
      "|org|org_idx|    org_dummy|\n",
      "+---+-------+-------------+\n",
      "|ORD|    0.0|(7,[0],[1.0])|\n",
      "|SFO|    1.0|(7,[1],[1.0])|\n",
      "|JFK|    2.0|(7,[2],[1.0])|\n",
      "|LGA|    3.0|(7,[3],[1.0])|\n",
      "|SMF|    4.0|(7,[4],[1.0])|\n",
      "|SJC|    5.0|(7,[5],[1.0])|\n",
      "|TUS|    6.0|(7,[6],[1.0])|\n",
      "|OGG|    7.0|    (7,[],[])|\n",
      "+---+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the one hot encoder class\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# Create an instance of the one hot encoder\n",
    "onehot = OneHotEncoder(inputCols=['org_idx'], outputCols=['org_dummy'])\n",
    "\n",
    "# Apply the one hot encoder to the flights data\n",
    "onehot = onehot.fit(flights)\n",
    "flights_onehot = onehot.transform(flights)\n",
    "\n",
    "# Check the results\n",
    "flights_onehot.select('org', 'org_idx', 'org_dummy').distinct().orderBy('org_idx').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c90673-5fbe-4b5d-aea8-4a7203b66a9d",
   "metadata": {},
   "source": [
    "### Encoding shirt sizes\n",
    "You have data for a consignment of t-shirts. The data includes the size of the shirt, which is given as either S, M, L or XL.\n",
    "\n",
    "Here are the counts for the different sizes:\n",
    "\n",
    "\n",
    "|size|count|\n",
    "|----|-----|\n",
    "|   S|    8|\n",
    "|   M|   15|\n",
    "|   L|   20|\n",
    "|  XL|    7|\n",
    "\n",
    "The sizes are first converted to an index using `StringIndexer` and then one-hot encoded using `OneHotEncoder`.\n",
    "\n",
    "Which of the following is not true:\n",
    "\n",
    "**Answer the question**\n",
    "\n",
    "- S shirts get index 2.0 and are one-hot encoded as (3,[2],[1.0])\n",
    "\n",
    "- M shirts get index 1.0 and are one-hot encoded as (3,[1],[1.0])\n",
    "\n",
    "- L shirts get index 0.0 and are one-hot encoded as (3,[0],[1.0])\n",
    "\n",
    "- **XL shirts get index 3.0 and are one-hot encoded as (3,[3],[1.0])**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8093e46-29ff-4717-8470-e6e36e5c5bba",
   "metadata": {},
   "source": [
    "## Regression\n",
    "1. Regression\n",
    "\n",
    "In the previous lesson you learned how to one-hot encode categorical features, which is essential for building regression models. In this lesson you'll find out how to build a regression model to predict numerical values.\n",
    "\n",
    "2. Consumption versus mass: scatter\n",
    "\n",
    "Returning to the cars data, suppose you wanted to predict fuel consumption using vehicle mass. A scatter plot is a good way to visualize the relationship between those two variables. Only a subset of the data are included in this plot, but it's clear that consumption increases with mass. However the relationship is not perfectly linear: there's scatter for individual points. A model should describe the average relationship of consumption to mass, without necessarily passing through individual points.\n",
    "\n",
    "3. Consumption versus mass: fit\n",
    "\n",
    "This line, for example, might describe the underlying trend in the data.\n",
    "\n",
    "4. Consumption versus mass: alternative fits\n",
    "\n",
    "But there are other lines which could equally well describe that trend. How do you choose the line which best describes the relationship?\n",
    "\n",
    "5. Consumption versus mass: residuals\n",
    "\n",
    "First we need to define the concept of residuals. The residual is the difference between the observed value and the corresponding modeled value. The residuals are indicated in the plot as the vertical lines between the data points and the model line. The best model would somehow make these residuals as small as possible.\n",
    "\n",
    "6. Loss function\n",
    "\n",
    "Out of all possible models, the best model is found by minimizing a loss function, which is an equation that describes how well the model fits the data. This is the equation for the mean squared error loss function. Let's quickly break it down.\n",
    "\n",
    "7. Loss function: Observed values\n",
    "\n",
    "You've got the observed values, y_i, …\n",
    "\n",
    "8. Loss function: Model values\n",
    "\n",
    "and the modeled values, \\hat{y}_i. The difference between these is the residual. The residuals are squared and then summed together…\n",
    "\n",
    "9. Loss function: Mean\n",
    "\n",
    "before finally dividing through by the number of data points to give the mean or average. By minimizing the loss function you are effectively minimizing the average residual or the average distance between the observed and modeled values. If this looks a little complicated, don't worry: Spark will do all of the maths for you.\n",
    "\n",
    "10. Assemble predictors\n",
    "\n",
    "Let's build a regression model to predict fuel consumption using three predictors: mass, number of cylinders and vehicle type, where the last is a categorical which we've already one-hot encoded. As before the first step towards building a model is to take our predictors and assemble them into a single column called 'features'. The data are then randomly split into training and testing sets.\n",
    "\n",
    "11. Build regression model\n",
    "\n",
    "The model is created using the LinearRegression class which is imported from the regression module. By default this class expects to find the target data in a column called \"label\". Since you are aiming to predict the \"consumption\" column you need to explicitly specify the name of the label column when creating a regression object. Next train the model on the training data using the fit() method. The trained model can then be used to making predictions on the testing data using the transform() method.\n",
    "\n",
    "12. Examine predictions\n",
    "\n",
    "Comparing the predicted values to the known values from the testing data you'll see that there is reasonable agreement. It's hard to tell from a table though. A plot gives a clearer picture. The dashed diagonal lie represents perfect prediction. Most of the points lie close to this line, which is good.\n",
    "\n",
    "13. Calculate RMSE\n",
    "\n",
    "It's useful to have a single number which summarizes the performance of a model. For classifiers there are a variety of such metrics. The Root Mean Squared Error is often used for regression models. It's the square root of the Mean Squared Error, which you've already encountered, and corresponds to the standard deviation of the residuals. The metrics for a classifier, like accuracy, precision and recall, are measured on an absolute scale where it's possible to immediately identify values that are \"good\" or \"bad\". Values of RMSE are relative to the scale of the value that you're aiming to predict, so interpretation is a little more challenging. A smaller RMSE, however, always indicates better predictions.\n",
    "\n",
    "14. Consumption versus mass: intercept\n",
    "\n",
    "Let's examine the model. The intercept is the value predicted by the model when all predictors are zero. On the plot this is the point where the model line intersects the vertical dashed line.\n",
    "\n",
    "15. Examine intercept\n",
    "\n",
    "You can find this value for the model using the intercept attribute. This is the predicted fuel consumption when both mass and number of cylinders are zero and the vehicle type is 'Van'. Of course, this is an entirely hypothetical scenario: no vehicle could have zero mass!\n",
    "\n",
    "16. Consumption versus mass: slope\n",
    "\n",
    "There's a slope associated with each of the predictors too, which represents how rapidly the model changes when that predictor changes.\n",
    "\n",
    "17. Examine Coefficients\n",
    "\n",
    "The coefficients attribute gives you access to those values. There's a coefficient for each of the predictors. The coefficients for mass and number of cylinders are positive, indicating that heavier cars with more cylinders consume more fuel. These coefficients also represent the rate of change for the corresponding predictor. For example, the coefficient for mass indicates the change in fuel consumption when mass increases by one unit. Remember that there's no dummy variable for Van? The coefficients for the type dummy variables are relative to Vans. These coefficients should also be interpreted with care: if you are going to compare the values for different vehicle types then this needs to be done for fixed mass and number of cylinders. Since all of the type dummy coefficients are negative, the model indicates that, for a specific mass and number of cylinders, all other vehicle types consume less fuel than a Van. Large vehicles have the most negative coefficient, so it's possible to say that, for a specific mass and number of cylinders, Large vehicles are the most fuel efficient.\n",
    "\n",
    "18. Regression for numeric predictions\n",
    "\n",
    "You've covered a lot of ground in this lesson. Let's apply what you've learned to the flights data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed335b0d-552f-4be7-807c-6f75e2d6bcb1",
   "metadata": {},
   "source": [
    "### Flight duration model: Just distance\n",
    "In this exercise you'll build a regression model to predict flight duration (the duration column).\n",
    "\n",
    "For the moment you'll keep the model simple, including only the distance of the flight (the `km` column) as a predictor.\n",
    "\n",
    "The data are in `flights`. The first few records are displayed in the terminal. These data have also been split into training and testing sets and are available as `flights_train` and `flights_test`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Create a linear regression object. Specify the name of the label column. Fit it to the training data.\n",
    "- Make predictions on the testing data.\n",
    "- Create a regression evaluator object and use it to evaluate `RMSE` on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fcf37b5c-b48e-49d7-a2a7-4f0ed27937ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+\n",
      "|   km|duration|features|\n",
      "+-----+--------+--------+\n",
      "|108.0|      43| [108.0]|\n",
      "|108.0|      43| [108.0]|\n",
      "|108.0|      44| [108.0]|\n",
      "|108.0|      44| [108.0]|\n",
      "|108.0|      44| [108.0]|\n",
      "+-----+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# No need to backup, last time we created a new variable\n",
    "assembler = VectorAssembler(inputCols = ['km'], outputCol = 'features')\n",
    "flights_train, flights_test = assembler.transform(flights.select('km', 'duration')).randomSplit([0.8,0.2], seed=42)\n",
    "flights_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42a869fd-9ba1-4621-98d0-8b548af6fd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|duration|prediction       |\n",
      "+--------+-----------------+\n",
      "|44      |52.32045015618229|\n",
      "|44      |52.32045015618229|\n",
      "|44      |52.32045015618229|\n",
      "|46      |52.32045015618229|\n",
      "|46      |52.32045015618229|\n",
      "+--------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16.881752082844525"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression = LinearRegression(labelCol = 'duration').fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "predictions = regression.transform(flights_test)\n",
    "predictions.select('duration', 'prediction').show(5, False)\n",
    "\n",
    "# Calculate the RMSE\n",
    "RegressionEvaluator(labelCol = 'duration').evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c1f771-b1d2-49a5-9539-81367da001f9",
   "metadata": {},
   "source": [
    "### Interpreting the coefficients\n",
    "The linear regression model for flight duration as a function of distance takes the form:\n",
    "\n",
    "$ \\text{duration} = \\alpha + \\beta \\times \\text{distance} $\n",
    "\n",
    "where\n",
    "\n",
    "- $\\alpha$ intercept (component of duration which does not depend on distance) and\n",
    "- $\\beta$ coefficient (rate at which duration increases as a function of distance; also called the slope).\n",
    "By looking at the coefficients of your model you will be able to infer\n",
    "\n",
    "how much of the average flight duration is actually spent on the ground and\n",
    "what the average speed is during a flight.\n",
    "The linear regression model is available as `regression`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- What's the intercept?\n",
    "- What are the coefficients? This is a vector.\n",
    "- Extract the element from the vector which corresponds to the slope for distance.\n",
    "- Find the average speed in km per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b1fe281c-a77d-4e90-859f-4a1af049cea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.147190398097976\n",
      "[0.07567833109337331]\n",
      "0.07567833109337331\n",
      "792.8293229137269\n"
     ]
    }
   ],
   "source": [
    "# Intercept (average minutes on ground)\n",
    "inter = regression.intercept\n",
    "print(inter)\n",
    "\n",
    "# Coefficients\n",
    "coefs = regression.coefficients\n",
    "print(coefs)\n",
    "\n",
    "# Average minutes per km\n",
    "minutes_per_km = regression.coefficients[0]\n",
    "print(minutes_per_km)\n",
    "\n",
    "# Average speed in km per hour - Divide the number of minutes in an hour by the average minutes per kilometer?\n",
    "avg_speed = 60 / minutes_per_km\n",
    "print(avg_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1d5376-bc84-42ca-ace2-85a9276f7d74",
   "metadata": {},
   "source": [
    "### Flight duration model: Adding origin airport\n",
    "Some airports are busier than others. Some airports are bigger than others too. Flights departing from large or busy airports are likely to spend more time taxiing or waiting for their takeoff slot. So it stands to reason that the duration of a flight might depend not only on the distance being covered but also on the airport from which the flight departs.\n",
    "\n",
    "You are going to make the regression model a little more sophisticated by including the departure airport as a predictor.\n",
    "\n",
    "These data have been split into training and testing sets and are available as `flights_train` and `flights_test`. The origin airport, stored in the `org` column, has been indexed into `org_idx`, which in turn has been one-hot encoded into `org_dummy`. The first few records are displayed in the terminal.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Fit a linear regression model to the training data.\n",
    "- Make predictions for the testing data.\n",
    "- Calculate the RMSE for predictions on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "471f8c21-aa2f-4c46-965a-c4bc3a4e17f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_backup6 = flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6da9cb9-aea3-4411-ab5f-dd20145712e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------+--------------------+\n",
      "|   km|    org_dummy|duration|            features|\n",
      "+-----+-------------+--------+--------------------+\n",
      "|108.0|(7,[0],[1.0])|      43|(8,[0,1],[108.0,1...|\n",
      "|108.0|(7,[0],[1.0])|      43|(8,[0,1],[108.0,1...|\n",
      "|108.0|(7,[0],[1.0])|      44|(8,[0,1],[108.0,1...|\n",
      "|108.0|(7,[0],[1.0])|      44|(8,[0,1],[108.0,1...|\n",
      "|108.0|(7,[0],[1.0])|      44|(8,[0,1],[108.0,1...|\n",
      "+-----+-------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols = ['km', 'org_dummy'], outputCol = 'features')\n",
    "flights_train, flights_test = assembler.transform(flights_onehot.select('km','org_dummy', 'duration')).randomSplit([0.8,0.2], seed = 42)\n",
    "flights_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf674355-c0d7-4f38-a8a8-ae756b90b0c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.965027847682057"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression = LinearRegression(labelCol = 'duration').fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data\n",
    "predictions = regression.transform(flights_test)\n",
    "\n",
    "# Calculate the RMSE on testing data\n",
    "RegressionEvaluator(labelCol = 'duration').evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d6aaae-897c-4bc6-a2b6-4eb571111b0d",
   "metadata": {},
   "source": [
    "### Interpreting coefficients\n",
    "Remember that origin airport, `org`, has eight possible values (`ORD`, `SFO`, `JFK`, `LGA`, `SMF`, `SJC`, `TUS` and `OGG`) which have been one-hot encoded to seven dummy variables in `org_dummy`.\n",
    "\n",
    "The values for `km` and `org_dummy` have been assembled into `features`, which has eight columns with sparse representation. Column indices in `features` are as follows:\n",
    "\n",
    "- 0 — `km`\n",
    "- 1 — `ORD`\n",
    "- 2 — `SFO`\n",
    "- 3 — `JFK`\n",
    "- 4 — `LGA`\n",
    "- 5 — `SMF`\n",
    "- 6 — `SJC` and\n",
    "- 7 — `TUS`.\n",
    "Note that `OGG` does not appear in this list because it is the reference level for the origin airport category.\n",
    "\n",
    "In this exercise, you'll be using the `intercept` and `coefficients` attributes to interpret the model.\n",
    "\n",
    "The `coefficients` attribute is a list, where the first element indicates how flight duration changes with flight distance.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Find the average speed in km per hour. This will be different to the value that you got earlier because your model is now more sophisticated.\n",
    "- What's the average time on the ground at `OGG`?\n",
    "- What's the average time on the ground at `JFK`?\n",
    "- What's the average time on the ground at `LGA`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "05d37063-8044-4ccf-84e6-0d2fd6f032f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807.7693956642808\n",
      "15.548047084068516\n",
      "68.22354950340909\n",
      "62.43652918727568\n"
     ]
    }
   ],
   "source": [
    "# Average speed in km per hour\n",
    "avg_speed_hour = 60 / regression.coefficients[0]\n",
    "print(avg_speed_hour)\n",
    "\n",
    "# Average minutes on ground at OGG\n",
    "inter = regression.intercept\n",
    "print(inter)\n",
    "\n",
    "# Average minutes on ground at JFK\n",
    "avg_ground_jfk = inter + regression.coefficients[3]\n",
    "print(avg_ground_jfk)\n",
    "\n",
    "# Average minutes on ground at LGA\n",
    "avg_ground_lga = inter + regression.coefficients[4]\n",
    "print(avg_ground_lga)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea6b0ff-72c2-44fe-bce6-4c36ef8cd158",
   "metadata": {},
   "source": [
    "## Bucketing & Engineering\n",
    "1. Bucketing & Engineering\n",
    "\n",
    "The largest improvements in Machine Learning model performance are often achieved by carefully manipulating features. In this lesson you'll be learning about a few approaches to doing this.\n",
    "\n",
    "2. Bucketing\n",
    "\n",
    "Let's start with bucketing. It's often convenient to convert a continuous variable, like age or height, into discrete values. This can be done by assigning values to buckets or bins with well defined boundaries. The buckets might have uniform or variable width.\n",
    "\n",
    "3. Bucketing heights\n",
    "\n",
    "Let's make this more concrete by thinking about observations of people's heights. If you plot the heights on a histogram then it seems reasonable…\n",
    "\n",
    "4. Bucketing heights\n",
    "\n",
    "… to divide the heights up into ranges. To each of these ranges…\n",
    "\n",
    "5. Bucketing heights\n",
    "\n",
    "… you assign a label. Then you create a new column in the data…\n",
    "\n",
    "6. Bucketing heights\n",
    "\n",
    "… with the appropriate labels. The resulting categorical variable is often a more powerful predictor than the original continuous variable.\n",
    "\n",
    "7. RPM histogram\n",
    "\n",
    "Let's apply this to the cars data. Looking at the distribution of values for RPM you see that the majority lie in the range between 4500 and 6000. There are a few either below or above this range. This suggests that it would make sense to bucket these values according to those boundaries.\n",
    "\n",
    "8. RPM buckets\n",
    "\n",
    "You create a bucketizer object, specifying the bin boundaries as the \"splits\" argument and also providing the names of the input and output columns. You then apply this object to the data by calling the transform() method.\n",
    "\n",
    "9. RPM buckets\n",
    "\n",
    "The result has a new column with the discrete bucket values. The three buckets have been assigned index values zero, one and two, corresponding to the low, medium and high ranges for RPM.\n",
    "\n",
    "10. One-hot encoded RPM buckets\n",
    "\n",
    "As you saw earlier, before you can use these index values in a regression model, they first need to be one-hot encoded. The low and medium RPM ranges are mapped to distinct dummy variables, while the high range is the reference level and does not get a separate dummy variable.\n",
    "\n",
    "11. Model with bucketed RPM\n",
    "\n",
    "Let's look at the intercept and coefficients for a model which predicts fuel consumption based on bucketed RPM data. The intercept tells us what the fuel consumption is for the reference level, which is the high RPM bucket. To get the consumption for the low RPM bucket you add the first coefficient to the intercept. Similarly, to find the consumption for the medium RPM bucket you add the second coefficient to the intercept.\n",
    "\n",
    "12. More feature engineering\n",
    "\n",
    "There are many other approaches to engineering new features. It's common to apply arithmetic operations to one or more columns to create new features.\n",
    "\n",
    "13. Mass & Height to BMI\n",
    "\n",
    "Returning to the heights data. Suppose that we also had data for mass.\n",
    "\n",
    "14. Mass & Height to BMI\n",
    "\n",
    "Then it might be perfectly reasonable to engineer a new column for BMI. Potentially BMI might be a more powerful predictor than either height or mass in isolation.\n",
    "\n",
    "15. Engineering density\n",
    "\n",
    "Let's apply this idea to the cars data. You have columns for mass and length. Perhaps some combination of the two might be even more meaningful. You can create different forms of density by dividing the mass through by the first three powers of length. Since you only have the length of the vehicles but not their width or height, the length is being used as a proxy for these missing dimensions. In so doing you create three new predictors. The first density represents how mass changes with vehicle length. The second and third densities approximate how mass varies with the area and volume of the vehicle. Which of these will be meaningful for our model? Right now you don't know, you're just trying things out. Powerful new features are often discovered through trial and error. In the next lesson you'll learn about a technique for selecting only the relevant predictors in a regression model.\n",
    "\n",
    "16. Let's engineer some features!\n",
    "\n",
    "Right now though, let's apply what you've learned to the flights data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac32ea0e-ef22-4a84-9490-296d6677bd32",
   "metadata": {},
   "source": [
    "### Bucketing departure time\n",
    "Time of day data are a challenge with regression models. They are also a great candidate for bucketing.\n",
    "\n",
    "In this lesson you will convert the flight departure times from numeric values between 0 (corresponding to 00:00) and 24 (corresponding to 24:00) to binned values. You'll then take those binned values and one-hot encode them.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Create a bucketizer object with bin boundaries at 0, 3, 6, …, 24 which correspond to times 0:00, 03:00, 06:00, …, 24:00. Specify input column as `depart` and output column as `depart_bucket`.\n",
    "- Bucket the departure times in the flights data. Show the first five values for `depart` and `depart_bucket`.\n",
    "- Create a one-hot encoder object. Specify output column as `depart_dummy`.\n",
    "- Train the encoder on the data and then use it to convert the bucketed departure times to dummy variables. Show the first five values for depart, `depart_bucket` and `depart_dummy`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4dad2f1e-8865-4934-a191-084b5b45393d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|depart|depart_bucket|\n",
      "+------+-------------+\n",
      "| 16.33|          5.0|\n",
      "|  6.17|          2.0|\n",
      "| 10.33|          3.0|\n",
      "|  7.98|          2.0|\n",
      "| 10.83|          3.0|\n",
      "+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+-------------+-------------+\n",
      "|depart|depart_bucket| depart_dummy|\n",
      "+------+-------------+-------------+\n",
      "| 16.33|          5.0|(7,[5],[1.0])|\n",
      "|  6.17|          2.0|(7,[2],[1.0])|\n",
      "| 10.33|          3.0|(7,[3],[1.0])|\n",
      "|  7.98|          2.0|(7,[2],[1.0])|\n",
      "| 10.83|          3.0|(7,[3],[1.0])|\n",
      "+------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer, OneHotEncoder\n",
    "\n",
    "# Create buckets at 3 hour intervals through the day\n",
    "buckets = Bucketizer(splits=[0,3,6,9,12,15,18,21,24], inputCol = 'depart', outputCol = 'depart_bucket')\n",
    "\n",
    "# Bucket the departure times\n",
    "bucketed = buckets.transform(flights)\n",
    "bucketed.select('depart', 'depart_bucket').show(5)\n",
    "\n",
    "# Create a one-hot encoder\n",
    "onehot = OneHotEncoder(inputCol = 'depart_bucket', outputCol = 'depart_dummy')\n",
    "\n",
    "# One-hot encode the bucketed departure times\n",
    "flights_onehot = onehot.fit(bucketed).transform(bucketed)\n",
    "flights_onehot.select('depart', 'depart_bucket', 'depart_dummy').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ba9f8-c6c9-4398-b960-489defc14c97",
   "metadata": {},
   "source": [
    "### Flight duration model: Adding departure time\n",
    "In the previous exercise the departure time was bucketed and converted to dummy variables. Now you're going to include those dummy variables in a regression model for flight duration.\n",
    "\n",
    "The data are in `flights`. The `km`, `org_dummy` and `depart_dummy` columns have been assembled into features, where `km` is index `0`, `org_dummy` runs from index `1` to `7` and `depart_dummy` from index 8 to 14.\n",
    "\n",
    "The data have been split into training and testing sets and a linear regression model, `regression`, has been built on the training data. Predictions have been made on the testing data and are available as predictions.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Find the RMSE for predictions on the testing data.\n",
    "- Find the average time spent on the ground for flights departing from `OGG` between 21:00 and 24:00.\n",
    "- Find the average time spent on the ground for flights departing from `OGG` between 03:00 and 06:00.\n",
    "- Find the average time spent on the ground for flights departing from `JFK` between 03:00 and 06:00."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83627f40-ac85-4426-a064-2eb3b0fb9cf2",
   "metadata": {},
   "source": [
    "Feature columns:\n",
    "\n",
    "* 0 — km\n",
    "* 1 — ORD\n",
    "* 2 — SFO\n",
    "* 3 — JFK\n",
    "* 4 — LGA\n",
    "* 5 — SJC\n",
    "* 6 — SMF\n",
    "* 7 — TUS\n",
    "* 8 — 00:00 to 03:00\n",
    "* 9 — 03:00 to 06:00\n",
    "* 10 — 06:00 to 09:00\n",
    "* 11 — 09:00 to 12:00\n",
    "* 12 — 12:00 to 15:00\n",
    "* 13 — 15:00 to 18:00\n",
    "* 14 — 18:00 to 21:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f25cf3dd-1429-4e0c-a65c-e76ae1916353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+-------------+-------------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|depart_bucket| depart_dummy|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+-------------+-------------+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|        0.0|    0.0|          5.0|(7,[5],[1.0])|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|        0.0|    1.0|          2.0|(7,[2],[1.0])|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|        1.0|    0.0|          3.0|(7,[3],[1.0])|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|        0.0|    1.0|          2.0|(7,[2],[1.0])|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|        1.0|    0.0|          3.0|(7,[3],[1.0])|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_onehot.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5d0e9919-7220-4d8a-b498-3704825b23d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-------------+-----------------------------+--------+\n",
      "|   km|    org_dummy| depart_dummy|                     features|duration|\n",
      "+-----+-------------+-------------+-----------------------------+--------+\n",
      "|108.0|(7,[0],[1.0])|    (7,[],[])|       (15,[0,1],[108.0,1.0])|      50|\n",
      "|108.0|(7,[0],[1.0])|    (7,[],[])|       (15,[0,1],[108.0,1.0])|      50|\n",
      "|108.0|(7,[0],[1.0])|(7,[2],[1.0])|(15,[0,1,10],[108.0,1.0,1.0])|      44|\n",
      "|108.0|(7,[0],[1.0])|(7,[2],[1.0])|(15,[0,1,10],[108.0,1.0,1.0])|      44|\n",
      "|108.0|(7,[0],[1.0])|(7,[2],[1.0])|(15,[0,1,10],[108.0,1.0,1.0])|      44|\n",
      "+-----+-------------+-------------+-----------------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "onehot = OneHotEncoder(inputCol = 'org_idx', outputCol = 'org_dummy')\n",
    "assembler = VectorAssembler(inputCols = ['km', 'org_dummy', 'depart_dummy'], outputCol = 'features')\n",
    "flights = onehot.fit(flights_onehot).transform(flights_onehot)\n",
    "flights_train, flights_test = assembler.transform(flights).select('km', 'org_dummy', 'depart_dummy', 'features', 'duration').randomSplit([0.8,0.2], seed = 42)\n",
    "flights_train.show(5, truncate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ba3bd280-de5e-4b7f-ba63-53bc0e4b4526",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = LinearRegression(labelCol = 'duration').fit(flights_train)\n",
    "predictions = regression.transform(flights_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fed9a96a-de46-4b19-aa19-aeb2ed93c9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test RMSE is 10.799923937159477\n",
      "10.257324622528868\n",
      "-3.3037768914902106\n",
      "63.31414109793149\n"
     ]
    }
   ],
   "source": [
    "# Find the RMSE on testing data\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "rmse = RegressionEvaluator(labelCol = 'duration').evaluate(predictions)\n",
    "print(\"The test RMSE is\", rmse)\n",
    "\n",
    "# Average minutes on ground at OGG for flights departing between 21:00 and 24:00\n",
    "avg_eve_ogg = regression.intercept\n",
    "print(avg_eve_ogg)\n",
    "\n",
    "# Average minutes on ground at OGG for flights departing between 03:00 and 06:00\n",
    "avg_night_ogg = regression.intercept + regression.coefficients[8]\n",
    "print(avg_night_ogg)\n",
    "\n",
    "# Average minutes on ground at JFK for flights departing between 03:00 and 06:00\n",
    "avg_night_jfk = regression.intercept + regression.coefficients[3] + regression.coefficients[9]\n",
    "print(avg_night_jfk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ce6464-09c5-4601-88b0-fa4bbadb3da2",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "1. Regularization\n",
    "\n",
    "The regression models that you've built up until now have blindly included all of the provided features. Next you are going to learn about a more sophisticated model which effectively selects only the most useful features.\n",
    "\n",
    "2. Features: Only a few\n",
    "\n",
    "A linear regression model attempts to derive a coefficient for each feature in the data. The coefficients quantify the effect of the corresponding features. More features imply more coefficients. This works well when your dataset has a few columns and many rows. You need to derive a few coefficients and you have plenty of data.\n",
    "\n",
    "3. Features: Too many\n",
    "\n",
    "The converse situation, many columns and few rows, is much more challenging. Now you need to calculate values for numerous coefficients but you don't have much data to do it. Even if you do manage to derive values for all of those coefficients, your model will end up being very complicated and difficult to interpret. Ideally you want to create a parsimonious model: one that has just the minimum required number of predictors. It will be as simple as possible, yet still able to make robust predictions.\n",
    "\n",
    "4. Features: Selected\n",
    "\n",
    "The obvious solution is to simply select the \"best\" subset of columns. But how to choose that subset? There are a variety of approaches to this \"feature selection\" problem.\n",
    "\n",
    "5. Loss function (revisited)\n",
    "\n",
    "In this lesson we'll be exploring one such approach to feature selection known as \"penalized regression\". The basic idea is that the model is penalized, or punished, for having too many coefficients. Recall that the conventional regression algorithm chooses coefficients to minimize the loss function, which is average of the squared residuals. A good model will result in low MSE because its predictions will be close to the observed values.\n",
    "\n",
    "6. Loss function with regularization\n",
    "\n",
    "With penalized regression an additional \"regularization\" or \"shrinkage\" term is added to the loss function. Rather than depending on the data, this term is a function of the model coefficients.\n",
    "\n",
    "7. Regularization term\n",
    "\n",
    "There are two standard forms for the regularization term. Lasso regression uses a term which is proportional to the absolute value of the coefficients, while Ridge regression uses the square of the coefficients. In both cases this extra term in the loss function penalizes models with too many coefficients. There's a subtle distinction between Lasso and Ridge regression. Both will shrink the coefficients of unimportant predictors. However, whereas Ridge will result in those coefficients being close to zero, Lasso will actually force them to zero precisely. It's also possible to have a mix of Lasso and Ridge. The strength of the regularization is determined by a parameter which is generally denoted by the Greek symbol lambda. When lambda = 0 there is no regularization and when lambda is large regularization completely dominates. Ideally you want to choose a value for lambda between these two extremes!\n",
    "\n",
    "8. Cars again\n",
    "\n",
    "Let's make this more concrete by returning to the cars data. We've assembled the mass, cylinders and type columns along with the freshly engineered density columns. We've effectively got ten predictors available for the model. As usual we'll split these data into training and testing sets.\n",
    "\n",
    "9. Cars: Linear regression\n",
    "03:54 - 04:25\n",
    "Let's start by fitting a standard linear regression model to the training data. You can then make predictions on the testing data and calculate the RMSE. When you look at the model coefficients you find that all predictors have been assigned non-zero values. This means that every predictor is contributing to the model. This is certainly possible, but it's unlikely that all of the features are actually important for predicting consumption.\n",
    "\n",
    "10. Cars: Ridge regression\n",
    "\n",
    "Now let's fit a Ridge Regression model to the same data. You get a Ridge Regression model by giving a value of zero for elasticNetParam. An arbitrary value of 0.1 has been chosen for the regularization strength. Later you'll learn a way to choose good values for this parameter based on the data. When you calculate the RMSE on the testing data you find that it has increased slightly, but not enough to cause concern. Looking at the coefficients you see that they are all smaller than the coefficients for the standard linear regression model. They have been \"shrunk\".\n",
    "\n",
    "11. Cars: Lasso regression\n",
    "\n",
    "Finally let's build a Lasso Regression model, by setting elasticNetParam to 1. Again you find that the testing RMSE has increased, but not by a significant degree. Turning to the coefficients though, you see that something important has happened: all but two of the coefficients are now zero. There are effectively only two predictors left in the model: the dummy variable for a small type car and the linear density. Lasso Regression has identified the most important predictors and set the coefficients for the rest to zero. This tells us that we can get a good model by simply knowing whether or not a car is 'small' and it's linear density. A simpler model with no significant loss in performance.\n",
    "\n",
    "12. Regularization ? simple model\n",
    "\n",
    "Let's try out regularization on our flight duration model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb418bd5-23f6-4815-bb1e-549bd33e850d",
   "metadata": {},
   "source": [
    "### Flight duration model: More features!\n",
    "Let's add more features to our model. This will not necessarily result in a better model. Adding some features might improve the model. Adding other features might make it worse.\n",
    "\n",
    "More features will always make the model more complicated and difficult to interpret.\n",
    "\n",
    "These are the features you'll include in the next model:\n",
    "\n",
    "- `km`\n",
    "- `org` (origin airport, one-hot encoded, 8 levels)\n",
    "- `depart` (departure time, binned in 3 hour intervals, one-hot encoded, 8 levels)\n",
    "- `dow` (departure day of week, one-hot encoded, 7 levels) and\n",
    "- `mon` (departure month, one-hot encoded, 12 levels).\n",
    "These have been assembled into the features column, which is a sparse representation of 32 columns (remember one-hot encoding produces a number of columns which is one fewer than the number of levels).\n",
    "\n",
    "The data are available as `flights`, randomly split into `flights_train` and `flights_test`.\n",
    "\n",
    "This exercise is based on a small subset of the flights data.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Fit a linear regression model to the training data.\n",
    "- Generate predictions for the testing data.\n",
    "- Calculate the RMSE on the testing data.\n",
    "- Look at the model coefficients. Are any of them zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9986e6dd-9863-4656-a6c8-30d4c0e103f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+-------------+-------------+-------------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|depart_bucket| depart_dummy|    org_dummy|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+-------------+-------------+-------------+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|        0.0|    0.0|          5.0|(7,[5],[1.0])|(7,[0],[1.0])|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|        0.0|    1.0|          2.0|(7,[2],[1.0])|(7,[1],[1.0])|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|        1.0|    0.0|          3.0|(7,[3],[1.0])|(7,[0],[1.0])|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|        0.0|    1.0|          2.0|(7,[2],[1.0])|(7,[1],[1.0])|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|        1.0|    0.0|          3.0|(7,[3],[1.0])|(7,[0],[1.0])|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+-------------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1896b9c8-8916-4523-93db-a1a9027e6cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+-------------+-------------+-------------+-------+-------------+-------+---------------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|depart_bucket| depart_dummy|    org_dummy|dow_idx|    dow_dummy|mon_idx|      mon_dummy|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+-------------+-------------+-------------+-------+-------------+-------+---------------+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|        0.0|    0.0|          5.0|(7,[5],[1.0])|(7,[0],[1.0])|    3.0|(6,[3],[1.0])|    2.0| (11,[2],[1.0])|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|        0.0|    1.0|          2.0|(7,[2],[1.0])|(7,[1],[1.0])|    2.0|(6,[2],[1.0])|    3.0| (11,[3],[1.0])|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|        1.0|    0.0|          3.0|(7,[3],[1.0])|(7,[0],[1.0])|    1.0|(6,[1],[1.0])|   10.0|(11,[10],[1.0])|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|        0.0|    1.0|          2.0|(7,[2],[1.0])|(7,[1],[1.0])|    1.0|(6,[1],[1.0])|    0.0| (11,[0],[1.0])|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|        1.0|    0.0|          3.0|(7,[3],[1.0])|(7,[0],[1.0])|    6.0|    (6,[],[])|    5.0| (11,[5],[1.0])|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+-------------+-------------+-------------+-------+-------------+-------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------------+--------+\n",
      "|              features|duration|\n",
      "+----------------------+--------+\n",
      "|      (32,[0],[161.0])|      34|\n",
      "|     (32,[0],[4001.0])|     302|\n",
      "|     (32,[0],[5316.0])|     399|\n",
      "|(32,[0,1],[280.0,1.0])|      55|\n",
      "|(32,[0,1],[985.0,1.0])|     104|\n",
      "+----------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dow_indexer = StringIndexer(inputCol = 'dow', outputCol = 'dow_idx')\n",
    "dow_enc = OneHotEncoder(inputCol = 'dow_idx', outputCol = 'dow_dummy')\n",
    "mon_indexer = StringIndexer(inputCol = 'mon', outputCol = 'mon_idx')\n",
    "mon_enc = OneHotEncoder(inputCol = 'mon_idx', outputCol = 'mon_dummy')\n",
    "flights_di = dow_indexer.fit(flights).transform(flights)\n",
    "flights_de = dow_enc.fit(flights_di).transform(flights_di)\n",
    "flights_mi = mon_indexer.fit(flights_de).transform(flights_de)\n",
    "flights_me = mon_enc.fit(flights_mi).transform(flights_mi)\n",
    "flights_me.show(5)\n",
    "assemb = VectorAssembler(inputCols = ['km', 'org_dummy', 'depart_dummy', 'dow_dummy', 'mon_dummy'], outputCol = 'features')\n",
    "flights_train, flights_test = assemb.transform(flights_me).select('features', 'duration').randomSplit([0.8, 0.2], seed = 42)\n",
    "flights_train.show(5, truncate = 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c7d52679-2746-4023-b949-b8da147dadf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test RMSE is 10.574746538111988\n",
      "[0.07439483222682225,27.83378904120845,20.663402810401653,51.90844432181796,46.15802944524765,15.428065698164207,17.958678934937222,17.751096954538646,-13.966840313924555,1.3614790076358905,4.113124317818998,7.026364390611397,4.651969172579217,8.896825045902881,8.717426904903839,0.07914809357208828,0.10699162263365765,0.29942494302441125,-0.0685932827822467,0.539004953229117,0.1506363578726905,-3.4468262069067883,-3.5863671211600536,-1.1770108756650566,-1.312471852247298,-1.5539893789442398,-3.690083530836403,0.9059503152070464,-3.3148944652132166,-2.7817574270519803,-3.2052505334180847,-2.1621402885887337]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Fit linear regression model to training data\n",
    "regression = LinearRegression(labelCol = 'duration').fit(flights_train)\n",
    "\n",
    "# Make predictions on testing data\n",
    "predictions = regression.transform(flights_test)\n",
    "\n",
    "# Calculate the RMSE on testing data\n",
    "rmse = RegressionEvaluator(labelCol = 'duration').evaluate(predictions)\n",
    "print(\"The test RMSE is\", rmse)\n",
    "\n",
    "# Look at the model coefficients\n",
    "coeffs = regression.coefficients\n",
    "print(coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32950a27-d8c0-4335-af84-86cdd8056bae",
   "metadata": {},
   "source": [
    "### Flight duration model: Regularization!\n",
    "In the previous exercise you added more predictors to the flight duration model. The model performed well on testing data, but with so many coefficients it was difficult to interpret.\n",
    "\n",
    "In this exercise you'll use Lasso regression (regularized with a L1 penalty) to create a more parsimonious model. Many of the coefficients in the resulting model will be set to zero. This means that only a subset of the predictors actually contribute to the model. Despite the simpler model, it still produces a good RMSE on the testing data.\n",
    "\n",
    "You'll use a specific value for the regularization strength. Later you'll learn how to find the best value using cross validation.\n",
    "\n",
    "The data (same as previous exercise) are available as `flights`, randomly split into `flights_train` and `flights_test`.\n",
    "\n",
    "There are two parameters for this model, `λ` (regParam) and `α` (elasticNetParam), where `α` determines the type of regularization and `λ` gives the strength of regularization.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Fit a linear regression model to the training data. Set the regularization strength to 1.\n",
    "- Calculate the RMSE on the testing data.\n",
    "- Look at the model coefficients.\n",
    "- How many of the coefficients are equal to zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "570a6e44-b638-4899-b179-86cdf786d180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test RMSE is 11.55850218375903\n",
      "[0.07348295337481496,5.6617372847641265,0.0,28.75266895221526,21.868371084613983,-2.3612686725471783,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0974936889170492,1.0833419175529913,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "Number of coefficients equal to 0: 56.17582720889105\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Fit Lasso model (λ = 1, α = 1) to training data\n",
    "regression = LinearRegression(labelCol = 'duration', regParam = 1, elasticNetParam=1).fit(flights_train)\n",
    "\n",
    "# Calculate the RMSE on testing data\n",
    "rmse = RegressionEvaluator(labelCol = 'duration').evaluate(regression.transform(flights_test))\n",
    "print(\"The test RMSE is\", rmse)\n",
    "\n",
    "# Look at the model coefficients\n",
    "coeffs = regression.coefficients\n",
    "print(coeffs)\n",
    "\n",
    "# Number of zero coefficients\n",
    "zero_coeff = sum([beta for beta in regression.coefficients])\n",
    "print(\"Number of coefficients equal to 0:\", zero_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd77dcd-01bb-4b8c-8e2c-3d238a166305",
   "metadata": {},
   "source": [
    "# Ensembles & Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f249b13-48d6-4559-a0a9-d312352250c7",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "1. Pipeline\n",
    "\n",
    "Welcome back! So far you've learned how to build classifier and regression models using Spark. In this chapter you'll learn how to make those models better. You'll start by taking a look at pipelines, which will seriously streamline your workflow. They will also help to ensure that training and testing data are treated consistently and that no leakage of information between these two sets takes place.\n",
    "\n",
    "2. Leakage?\n",
    "\n",
    "What do I mean by leakage? Most of the actions you've been using involve both a fit() and a transform() method. Those methods have been applied in a fairly relaxed way. But to get really robust results you need to be careful only to apply the fit() method to training data. Why? Because if a fit() method is applied to *any* of the testing data then the model will effectively have seen those data during the training phase, so the results of testing will no longer be objective. The transform() method, on the other hand, can be applied to both training and testing data since it does not result in any changes in the underlying model.\n",
    "\n",
    "3. A leaky model\n",
    "\n",
    "A figure should make this clearer. Leakage occurs whenever a fit() method is applied to testing data. Suppose that you fit a model using both the training and testing data. The model would then already have *seen* the testing data, so using those data to test the model would not be fair: of course the model will perform well on data which has been used for training! This sounds obvious, but care must be taken not to fall into this trap. Remember that there are normally multiple stages in building a model and if the fit() method in *any* of those stages is applied to the testing data then the model is compromised.\n",
    "\n",
    "4. A watertight model\n",
    "\n",
    "However, if you are careful to only apply fit() to the training data then your model will be in good shape. When it comes to testing it will not have seen *any* of the testing data and the test results will be completely objective. Luckily a pipeline will make it easier to avoid leakage because it simplifies the training and testing process.\n",
    "\n",
    "5. Pipeline\n",
    "\n",
    "A pipeline is a mechanism to combine a series of steps. Rather than applying each of the steps individually, they are all grouped together and applied as a single unit.\n",
    "\n",
    "6. Cars model: Steps\n",
    "\n",
    "Let's return to our cars regression model. Recall that there were a number of steps involved: - using a string indexer to convert the type column to indexed values; - applying a one-hot encoder to convert those indexed values into dummy variables; then - assembling a set of predictors into a single features column; and finally - building a regression model.\n",
    "\n",
    "7. Cars model: Applying steps\n",
    "\n",
    "Let's map out the process of applying those steps. - First you fit the indexer to the training data. Then you call the transform() method on the training data to add the indexed column. - Then you call the transform() method on the testing data to add the indexed column there too. Note that the testing data was not used to fit the indexer. Next you do the same things for the one-hot encoder, fitting to the training data and then using the fitted encoder to update the training and testing data sets. The assembler is next. In this case there is no fit() method, so you simply apply the transform() method to the training and testing data. Finally the data are ready. You fit the regression model to the training data and then use the model to make predictions on the testing data. Throughout the process you've been careful to keep the testing data out of the training process. But this is hard work and it's easy enough to slip up.\n",
    "\n",
    "8. Cars model: Pipeline\n",
    "\n",
    "A pipeline makes training and testing a complicated model a lot easier. The Pipeline class lives in the ml sub-module. You create a pipeline by specifying a sequence of stages, where each stage corresponds to a step in the model building process. The stages are executed in order. Now, rather than calling the fit() and transform() methods for each stage, you simply call the fit() method for the pipeline on the training data. Each of the stages in the pipeline is then automatically applied to the training data in turn. This will systematically apply the fit() and transform() methods for each stage in the pipeline. The trained pipeline can then be used to make predictions on the testing data by calling its transform() method. The pipeline transform() method will only call the transform() method for each of the stages in the pipeline. Isn't that simple?\n",
    "\n",
    "9. Cars model: Stages\n",
    "\n",
    "You can access the stages in the pipeline by using the .stages attribute, which is a list. You pick out individual stages by indexing into the list. For example, to access the regression component of the pipeline you'd use an index of 3. Having access to that component makes it possible to get the intercept and coefficients for the trained LinearRegression model.\n",
    "\n",
    "10. Pipelines streamline workflow!\n",
    "\n",
    "Pipelines make your code easier to read and maintain. Let's try them out with our flights model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7052d2",
   "metadata": {},
   "source": [
    "### Flight duration model: Pipeline stages\n",
    "You're going to create the stages for the flights duration model pipeline. You will use these in the next exercise to build a pipeline and to create a regression model.\n",
    "\n",
    "The `StringIndexer`, `OneHotEncoder`, `VectorAssembler` and `LinearRegression` classes are already imported.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Create an indexer to convert the `'org'` column into an indexed column called `'org_idx'`.\n",
    "- Create a one-hot encoder to convert the `'org_idx'` and `'dow'` columns into dummy variable columns called `'org_dummy'` and `'dow_dummy'`.\n",
    "- Create an assembler which will combine the `'km'` column with the two dummy variable columns. The output column should be called `'features'`.\n",
    "- Create a linear regression object to predict flight duration.\n",
    "\n",
    "You might find it useful to revisit the slides from the lessons in the Slides panel next to the IPython Shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5aeaad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical strings to index values\n",
    "indexer = StringIndexer(inputCol = 'org', outputCol = 'org_idx')\n",
    "\n",
    "# One-hot encode index values\n",
    "onehot = OneHotEncoder(\n",
    "    inputCols=['org_idx', 'dow'],\n",
    "    outputCols=['org_dummy', 'dow_dummy']\n",
    ")\n",
    "\n",
    "# Assemble predictors into a single column\n",
    "assembler = VectorAssembler(inputCols=['km','org_dummy','dow_dummy'], outputCol='features')\n",
    "\n",
    "# A linear regression object\n",
    "regression = LinearRegression(labelCol='duration')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bed11ba",
   "metadata": {},
   "source": [
    "### Flight duration model: Pipeline model\n",
    "You're now ready to put those stages together in a pipeline.\n",
    "\n",
    "You'll construct the pipeline and then train the pipeline on the training data. This will apply each of the individual stages in the pipeline to the training data in turn. None of the stages will be exposed to the testing data at all: there will be no leakage!\n",
    "\n",
    "Once the entire pipeline has been trained it will then be used to make predictions on the testing data.\n",
    "\n",
    "The data are available as `flights`, which has been randomly split into `flights_train` and `flights_test`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import the class for creating a pipeline.\n",
    "- Create a pipeline object and specify the `indexer`, `onehot`, `assembler` and `regression` stages, in this order.\n",
    "- Train the pipeline on the training data.\n",
    "- Make predictions on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8df205a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+-------------+-------------+-------------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|depart_bucket| depart_dummy|    org_dummy|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+-------------+-------------+-------------+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|        0.0|    0.0|          5.0|(7,[5],[1.0])|(7,[0],[1.0])|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|        0.0|    1.0|          2.0|(7,[2],[1.0])|(7,[1],[1.0])|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|        1.0|    0.0|          3.0|(7,[3],[1.0])|(7,[0],[1.0])|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|        0.0|    1.0|          2.0|(7,[2],[1.0])|(7,[1],[1.0])|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|        1.0|    0.0|          3.0|(7,[3],[1.0])|(7,[0],[1.0])|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+-------------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "455113ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|        0.0|    0.0|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|        0.0|    1.0|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|        1.0|    0.0|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|        0.0|    1.0|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|        1.0|    0.0|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_backup3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d8b3f9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|  0|  1|  2|     AA|JFK|  12.0|     370|   11|3983.0|    0|\n",
      "|  0|  1|  2|     AA|LGA|  9.92|     170|   -9|1180.0|    0|\n",
      "|  0|  1|  2|     AA|LGA| 20.42|     185|   31|1765.0|    1|\n",
      "|  0|  1|  2|     AA|ORD|  9.08|     560|   39|6828.0|    1|\n",
      "|  0|  1|  2|     AA|ORD| 14.08|     270|   20|3335.0|    1|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_test, flights_train = flights_backup3.drop('carrier_idx').drop('org_idx').randomSplit([0.8,0.2], seed = 42)\n",
    "flights_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "891ed741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import class for creating a pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Construct a pipeline\n",
    "pipeline = Pipeline(stages=[indexer, onehot, assembler, regression])\n",
    "\n",
    "# Train the pipeline on the training data\n",
    "pipeline = pipeline.fit(flights_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = pipeline.transform(flights_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ab502b",
   "metadata": {},
   "source": [
    "### SMS spam pipeline\n",
    "You haven't looked at the SMS data for quite a while. Last time we did the following:\n",
    "\n",
    "- split the text into tokens\n",
    "- removed stop words\n",
    "- applied the hashing trick\n",
    "- converted the data from counts to IDF and\n",
    "- trained a logistic regression model.\n",
    "Each of these steps was done independently. This seems like a great application for a pipeline!\n",
    "\n",
    "The `Pipeline` and `LogisticRegression` classes have already been imported into the session, so you don't need to worry about that!\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Create an object for splitting text into tokens.\n",
    "- Create an object to remove stop words. Rather than explicitly giving the input column name, use the `getOutputCol()` method on the previous object.\n",
    "- Create objects for applying the hashing trick and transforming the data into a TF-IDF. Use the `getOutputCol()` method again.\n",
    "- Create a pipeline which wraps all of the above steps as well as an object to create a Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a3d72323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "# Break text into tokens at non-word characters\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='terms')\n",
    "\n",
    "# Apply the hashing trick and transform to TF-IDF\n",
    "hasher = HashingTF(inputCol=remover.getOutputCol(), outputCol=\"hash\")\n",
    "idf = IDF(inputCol=hasher.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "# Create a logistic regression object and add everything to a pipeline\n",
    "logistic = LogisticRegression()\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hasher, idf, logistic])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847feb1-29b0-461c-b229-961d181f9e6e",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "1. Cross-Validation\n",
    "\n",
    "Up until now you've been testing models using a rather simple technique: randomly splitting the data into training and testing sets, training the model on the training data and then evaluating its performance on the testing set. There's one major drawback to this approach: you only get one estimate of the model performance. You would have a more robust idea of how well a model works if you were able to test it multiple times. This is precisely the idea behind cross-validation.\n",
    "\n",
    "2. CV - complete data\n",
    "\n",
    "You start out with the full set of data.\n",
    "\n",
    "3. CV - train/test split\n",
    "\n",
    "You still split these data into a training set and a testing set. Remember that before splitting it's important to first randomize the data so that the distributions in the training and testing data are similar.\n",
    "\n",
    "4. CV - multiple folds\n",
    "\n",
    "You then split the training data into a number of partitions or \"folds\". The number of folds normally factors into the name of the technique. For example, if you split into five folds then you'd talk about 5-fold cross-validation.\n",
    "\n",
    "5. Fold upon fold - first fold\n",
    "\n",
    "Once the training data have been split into folds you can start cross-validating. First keep aside the data in the first fold. Train a model on the remaining four folds. Then evaluate that model on the data from the first fold. This will give the first value for the evaluation metric.\n",
    "\n",
    "6. Fold upon fold - second fold\n",
    "\n",
    "Next you move onto the second fold, where the same process is repeated: data in the second fold are set aside for testing while the remaining four folds are used to train a model. That model is tested on the second fold data, yielding the second value for the evaluation metric.\n",
    "\n",
    "7. Fold upon fold - other folds\n",
    "\n",
    "You repeat the process for the remaining folds. Each of the folds is used in turn as testing data and you end up with as many values for the evaluation metric as there are folds. At this point you are in a position to calculate the average of the evaluation metric over all folds, which is a much more robust measure of model performance than a single value.\n",
    "\n",
    "8. Cars revisited\n",
    "\n",
    "Let's see how this works in practice. Remember the cars data? Of course you do. You're going to build a cross-validated regression model to predict consumption.\n",
    "\n",
    "9. Estimator and evaluator\n",
    "\n",
    "Here are the first two ingredients which you need to perform cross-validation: - an estimator, which builds the model and is often a pipeline; and - an evaluator, which quantifies how well a model works on testing data. We've seen both of these a few times already.\n",
    "\n",
    "10. Grid and cross-validator\n",
    "\n",
    "Now the final ingredients. You'll need two new classes, CrossValidator and ParamGridBuilder, both from the tuning sub-module. You'll create a parameter grid, which you'll leave empty for the moment, but will return to in detail during the next lesson. Finally you have everything required to create a cross-validator object: - an estimator, which is the linear regression model, - an empty grid of parameters for the estimator and - an evaluator which will calculate the RMSE. You can optionally specify the number of folds (which defaults to three) and a random number seed for repeatability.\n",
    "\n",
    "11. Cross-validators need training too\n",
    "\n",
    "The cross-validator has a fit() method which will apply the cross-validation procedure to the training data. You can then look at the average RMSE calculated across all of the folds. This is a more robust measure of model performance because it is based on multiple train/test splits. Note that the average metric is returned as a list. You'll see why in the next lesson.\n",
    "\n",
    "12. Cross-validators act like models\n",
    "\n",
    "The trained cross-validator object acts just like any other model. It has a transform method, which can be used to make predictions on new data. If we evaluate the predictions on the original testing data then we find a smaller value for the RMSE than we obtained using cross-validation. This means that a simple train-test split would have given an overly optimistic view on model performance.\n",
    "\n",
    "13. Cross-validate all the models!\n",
    "\n",
    "Let's give cross-validation a try on our flights model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9bbe68",
   "metadata": {},
   "source": [
    "### Cross validating simple flight duration model\n",
    "You've already built a few models for predicting flight duration and evaluated them with a simple train/test split. However, cross-validation provides a much better way to evaluate model performance.\n",
    "\n",
    "In this exercise you're going to train a simple model for flight duration using cross-validation. Travel time is usually strongly correlated with distance, so using the `km` column alone should give a decent model.\n",
    "\n",
    "The data have been randomly split into `flights_train` and `flights_test`.\n",
    "\n",
    "The following classes have already been imported: `LinearRegression`, `RegressionEvaluator`, `ParamGridBuilder` and `CrossValidator`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Create an empty parameter grid.\n",
    "- Create objects for building and evaluating a linear regression model. The model should predict the `\"duration\"` field.\n",
    "- Create a cross-validator object. Provide values for the `estimator`, `estimatorParamMaps` and `evaluator` arguments. Choose 5-fold cross validation.\n",
    "- Train and test the model across multiple folds of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "92ecd84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "76b94ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assemb = VectorAssembler(inputCols = ['km'], outputCol = 'features')\n",
    "flights_train = assemb.transform(flights_train.drop('features'))\n",
    "flights_test = assemb.transform(flights_test.drop('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dc16c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty parameter grid\n",
    "params = ParamGridBuilder().build()\n",
    "\n",
    "# Create objects for building and evaluating a regression model\n",
    "regression = LinearRegression(labelCol = 'duration')\n",
    "evaluator = RegressionEvaluator(labelCol = 'duration')\n",
    "\n",
    "# Create a cross validator\n",
    "cv = CrossValidator(estimator=regression, estimatorParamMaps=params, evaluator=evaluator, numFolds = 5)\n",
    "\n",
    "# Train and test model on multiple folds of the training data\n",
    "cv = cv.fit(flights_train)\n",
    "\n",
    "# NOTE: Since cross-valdiation builds multiple models, the fit() method can take a little while to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8a88d6",
   "metadata": {},
   "source": [
    "### Cross validating flight duration model pipeline\n",
    "The cross-validated model that you just built was simple, using km alone to predict duration.\n",
    "\n",
    "Another important predictor of flight duration is the origin airport. Flights generally take longer to get into the air from busy airports. Let's see if adding this predictor improves the model!\n",
    "\n",
    "In this exercise you'll add the `org` field to the model. However, since `org` is categorical, there's more work to be done before it can be included: it must first be transformed to an index and then one-hot encoded before being assembled with `km` and used to build the regression model. We'll wrap these operations up in a pipeline.\n",
    "\n",
    "The following objects have already been created:\n",
    "\n",
    "- `params` — an empty parameter grid\n",
    "- `evaluator` — a regression evaluator\n",
    "- `regression` — a LinearRegression object with labelCol='duration'.\n",
    "\n",
    "The `StringIndexer`, `OneHotEncoder`, `VectorAssembler` and `CrossValidator` classes have already been imported.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Create a string indexer. Specify the input and output fields as `org` and `org_idx`.\n",
    "- Create a one-hot encoder. Name the output field `org_dummy`.\n",
    "- Assemble the `km` and `org_dummy` fields into a single field called `features`.\n",
    "- Create a pipeline using the following operations: string indexer, one-hot encoder, assembler and linear regression. Use this to create a cross-validator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b0a91c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an indexer for the org field\n",
    "indexer = StringIndexer(inputCol = 'org', outputCol = 'org_idx')\n",
    "\n",
    "# Create an one-hot encoder for the indexed org field\n",
    "onehot = OneHotEncoder(inputCols = ['org_idx'], outputCols = ['org_dummy'])\n",
    "\n",
    "# Assemble the km and one-hot encoded fields\n",
    "assembler = VectorAssembler(inputCols = ['km', 'org_dummy'], outputCol = 'features')\n",
    "\n",
    "# Create a pipeline and cross-validator.\n",
    "pipeline = Pipeline(stages=[indexer, onehot, assembler, regression])\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "          estimatorParamMaps=params,\n",
    "          evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa58f1d-a9eb-4162-9bbd-031f3652963a",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "1. Grid Search\n",
    "\n",
    "So far you've been using the default parameters for almost everything. You've built some decent models, but they could probably be improved by choosing better model parameters.\n",
    "\n",
    "2. Tuning\n",
    "\n",
    "There is no universal \"best\" set of parameters for a particular model. The optimal choice of parameters will depend on the data and the modeling goal. The idea is relatively simple, you build a selection of models, one for each set of model parameters. Then you evaluate those models and choose the best one.\n",
    "\n",
    "3. Cars revisited (again)\n",
    "\n",
    "You'll be looking at the fuel consumption regression model again.\n",
    "\n",
    "4. Fuel consumption with intercept\n",
    "\n",
    "You'll start by doing something simple, comparing a linear regression model with an intercept to one that passes through the origin. By default a linear regression model will always fit an intercept, but you're going to be explicit and specify the fitIntercept parameter as True. You fit the model to the training data and then calculate the RMSE for the testing data.\n",
    "\n",
    "5. Fuel consumption without intercept\n",
    "\n",
    "Next you repeat the process, but specify False for the fitIntercept parameter. Now you are creating a model which passes through the origin. When you evaluate this model you find that the RMSE is higher. So, comparing these two models you'd naturally choose the first one because it has a lower RMSE. However, there's a problem with this approach. Just getting a single estimate of RMSE is not very robust. It'd be better to make this comparison using cross-validation. You also have to manually build the models for the two different parameter values. It'd be great if that were automated.\n",
    "\n",
    "6. Parameter grid\n",
    "\n",
    "You can systematically evaluate a model across a grid of parameter values using a technique known as grid search. To do this you need to set up a parameter grid. You actually saw this in the previous lesson, where you simply created an empty grid. Now you are going to add points to the grid. First you create a grid builder and then you add one or more grids. At present there's just one grid, which takes two values for the fitIntercept parameter. Call the build() method to construct the grid. A separate model will be built for each point in the grid. You can check how many models this corresponds to and, of course, this is just two.\n",
    "\n",
    "7. Grid search with cross-validation\n",
    "\n",
    "Now you create a cross-validator object and fit it to the training data. This builds a bunch of models: one model for each fold and point in the parameter grid. Since there are two points in the grid and ten folds, this translates into twenty models. The cross-validator is going to loop through each of the points in the parameter grid and for each point it will create a cross-validated model using the corresponding parameter values. When you take a look at the average metrics attribute, you can see why the metric is given as a list: you get one average value for each point in the grid. The values confirm what you observed before: the model that includes an intercept is superior to the model without an intercept.\n",
    "\n",
    "8. The best model & parameters\n",
    "\n",
    "Our goal was to get the best model for the data. You retrieve this using the appropriately named bestModel attribute. But it's not actually necessary to work with this directly because the cross-validator object will behave like the best model. So, you can use it directly to make predictions on the testing data. Of course, you want to know what the best parameter value is and you can retrieve this using the explainParam() method. As expected the best value for the fitIntercept parameter is True. You can see this after the word \"current\" in the output.\n",
    "\n",
    "9. A more complicated grid\n",
    "\n",
    "It's possible to add more parameters to the grid. Here, in addition to whether or not to include an intercept, you're also considering a selection of values for the regularization parameter and the elastic net parameter. Of course, the more parameters and values you add to the grid, the more models you have to evaluate. Because each of these models will be evaluated using cross-validation, this might take a little while. But it will be time well spent, because the model that you get back will in principle be much better than what you would have obtained by just using the default parameters.\n",
    "\n",
    "10. Find the best parameters!\n",
    "\n",
    "Let's apply grid search on the flights and SMS models!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87cd39",
   "metadata": {},
   "source": [
    "### Optimizing flights linear regression\n",
    "Up until now you've been using the default hyper-parameters when building your models. In this exercise you'll use cross validation to choose an optimal (or close to optimal) set of model hyper-parameters.\n",
    "\n",
    "The following have already been created:\n",
    "\n",
    "- `regression` — a `LinearRegression` object\n",
    "- `pipeline` — a pipeline with string indexer, one-hot encoder, vector assembler and linear regression and\n",
    "- `evaluator` — a `RegressionEvaluator` object.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Create a parameter grid builder.\n",
    "- Add grids for with `regression.regParam` (values `0.01`, `0.1`, `1.0`, and `10.0`) and `regression.elasticNetParam` (values `0.0`, `0.5`, and `1.0`).\n",
    "- Build the grid.\n",
    " Create a cross validator, specifying five folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fc4fb81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models to be tested:  12\n"
     ]
    }
   ],
   "source": [
    "# Create parameter grid\n",
    "params = ParamGridBuilder()\n",
    "\n",
    "# Add grids for two parameters\n",
    "params = params.addGrid(regression.regParam, [0.01, 0.1,1.0,10.0]) \\\n",
    "               .addGrid(regression.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "\n",
    "# Build the parameter grid\n",
    "params = params.build()\n",
    "print('Number of models to be tested: ', len(params))\n",
    "\n",
    "# Create cross-validator\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=params, evaluator=evaluator, numFolds = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71387b32",
   "metadata": {},
   "source": [
    "### Dissecting the best flight duration model\n",
    "You just set up a CrossValidator to find good parameters for the linear regression model predicting flight duration.\n",
    "\n",
    "The model pipeline has multiple stages (objects of type `StringIndexer`, `OneHotEncoder`, `VectorAssembler` and `LinearRegression`), which operate in sequence. The stages are available as the stages attribute on the pipeline object. They are represented by a list and the stages are executed in the sequence in which they appear in the list.\n",
    "\n",
    "Now you're going to take a closer look at the pipeline, split out the stages and use it to make predictions on the testing data.\n",
    "\n",
    "The following objects have already been created:\n",
    "\n",
    "- `cv` — a trained `CrossValidatorModel` object and\n",
    "- `evaluator` — a `RegressionEvaluator` object.\n",
    "- The flights data have been randomly split into `flights_train` and `flights_test`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Retrieve the best model.\n",
    "- Look at the stages in the best model.\n",
    "- Isolate the linear regression stage and extract its parameters.\n",
    "- Use the best model to generate predictions on the testing data and calculate the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0b015c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = cv.fit(flights_train.drop('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6b5413b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StringIndexerModel: uid=StringIndexer_1328fc2dcd3d, handleInvalid=error, OneHotEncoderModel: uid=OneHotEncoder_2363f0554273, dropLast=true, handleInvalid=error, numInputCols=1, numOutputCols=1, VectorAssembler_324580bf5543, LinearRegressionModel: uid=LinearRegression_5e483bc6ded0, numFeatures=8]\n",
      "RMSE = 11.064032706878418\n"
     ]
    }
   ],
   "source": [
    "# Get the best model from cross validation\n",
    "best_model = cv.bestModel\n",
    "\n",
    "# Look at the stages in the best model\n",
    "print(best_model.stages)\n",
    "\n",
    "# Get the parameters for the LinearRegression object in the best model\n",
    "best_model.stages[3].extractParamMap()\n",
    "\n",
    "# Generate predictions on testing data using the best model then calculate RMSE\n",
    "predictions = best_model.transform(flights_test.drop('features'))\n",
    "print(\"RMSE =\", evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1140a81",
   "metadata": {},
   "source": [
    "### SMS spam optimised\n",
    "The pipeline you built earlier for the SMS spam model used the default parameters for all of the elements in the pipeline. It's very unlikely that these parameters will give a particularly good model though. In this exercise you're going to run the pipeline for a selection of parameter values. We're going to do this in a systematic way: the values for each of the parameters will be laid out on a grid and then pipeline will systematically run across each point in the grid.\n",
    "\n",
    "In this exercise you'll set up a parameter grid which can be used with cross validation to choose a good set of parameters for the SMS spam classifier.\n",
    "\n",
    "The following are already defined:\n",
    "\n",
    "- hasher — a HashingTF object and\n",
    "- logistic — a LogisticRegression object.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Create a parameter grid builder object.\n",
    "- Add grid points for `numFeatures` and `binary` parameters to the `HashingTF` object, giving values 1024, 4096 and 16384, and True and False, respectively.\n",
    "- Add grid points for `regParam` and `elasticNetParam` parameters to the `LogisticRegression` object, giving values of 0.01, 0.1, 1.0 and 10.0, and 0.0, 0.5, and 1.0 respectively.\n",
    "- Build the parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "19c8d045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parameter grid\n",
    "params = ParamGridBuilder()\n",
    "\n",
    "# Add grid for hashing trick parameters\n",
    "params = params.addGrid(hasher.numFeatures, [1024, 4096, 16384]) \\\n",
    "               .addGrid(hasher.binary, [True, False])\n",
    "\n",
    "# Add grid for logistic regression parameters\n",
    "params = params.addGrid(logistic.regParam, [0.01, 0.1, 1.0, 10.0]) \\\n",
    "               .addGrid(logistic.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "\n",
    "# Build parameter grid\n",
    "params = params.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a565df44",
   "metadata": {},
   "source": [
    "### How many models for grid search?\n",
    "How many models will be built when the cross-validator below is fit to data?\n",
    "\n",
    "`params = ParamGridBuilder().addGrid(hasher.numFeatures, [1024, 4096, 16384]) \\\n",
    "                           .addGrid(hasher.binary, [True, False]) \\\n",
    "                           .addGrid(logistic.regParam, [0.01, 0.1, 1.0, 10.0]) \\\n",
    "                           .addGrid(logistic.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "                           .build()\n",
    "\n",
    "cv = CrossValidator(..., estimatorParamMaps=params, numFolds=5)`\n",
    "\n",
    "Possible Answers\n",
    "\n",
    "- 3\n",
    "\n",
    "- 5\n",
    "\n",
    "- 72\n",
    "\n",
    "- **360**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155685bb-f833-43c5-a547-119ca115372f",
   "metadata": {},
   "source": [
    "## Ensemble\n",
    "1. Ensemble\n",
    "\n",
    "You now know how to choose a good set of parameters for any model using cross-validation and grid search. In the final lesson you're going to learn about how models can be combined to form a collection or \"ensemble\" which is more powerful than each of the individual models alone.\n",
    "\n",
    "2. What's an ensemble?\n",
    "\n",
    "Simply put, an ensemble model is just a collection of models. An ensemble model combines the results from multiple models to produce better predictions than any one of those models acting alone. The concept is based on the idea of the \"Wisdom of the Crowd\", which implies that the aggregated opinion of a group is better than the opinions of the individuals in that group, even if the individuals are experts.\n",
    "\n",
    "3. Ensemble diversity\n",
    "\n",
    "As the quote suggests, for this idea to be true, there must be diversity and independence in the crowd. This applies to models too: a successful ensemble requires diverse models. It does not help if all of the models in the ensemble are similar or exactly the same. Ideally each of the models in the ensemble should be different.\n",
    "\n",
    "4. Random Forest\n",
    "\n",
    "A Random Forest, as the name implies, is a collection of trees. To ensure that each of those trees is different, the Decision Tree algorithm is modified slightly: - each tree is trained on a different random subset of the data and - within each tree a random subset of features is used for splitting at each node. The result is a collection of trees where no two trees are the same. Within the Random Forest model, all of the trees operate in parallel.\n",
    "\n",
    "5. Create a forest of trees\n",
    "\n",
    "Let's go back to the cars classifier yet again. You create a Random Forest model using the RandomForestClassifier class from the classification sub-module. You can select the number of trees in the forest using the numTrees parameter. By default this is twenty, but we'll drop that to five so that the results are easier to interpret. As is the case with any other model, the Random Forest is fit to the training data.\n",
    "\n",
    "6. Seeing the trees\n",
    "\n",
    "Once the model is trained it's possible to access the individual trees in the forest using the trees attribute. You would not normally do this, but it's useful for illustrative purposes. There are precisely five trees in the forest, as specified. The trees are all different, as can be seen from the varying number of nodes in each tree. You can then make predictions using each tree individually.\n",
    "\n",
    "7. Predictions from individual trees\n",
    "\n",
    "Here are the predictions of individual trees on a subset of the testing data. Each row represents predictions from each of the five trees for a specific record. In some cases all of the trees agree, but there is often some dissent amongst the models. This is precisely where the Random Forest works best: where the prediction is not clear cut. The Random Forest model creates a consensus prediction by aggregating the predictions across all of the individual trees.\n",
    "\n",
    "8. Consensus predictions\n",
    "\n",
    "You don't need to worry about these details though because the transform() method will automatically generate a consensus prediction column. It also creates a probability column which assigns aggregate probabilities to each of the outcomes.\n",
    "\n",
    "9. Feature importances\n",
    "\n",
    "It's possible to get an idea of the relative importance of the features in the model by looking at the featureImportances attribute. An importance is assigned to each feature, where a larger importance indicates a feature which makes a larger contribution to the model. Looking carefully at the importances we see that feature 4 (rpm) is the most important, while feature 0 (the number of cylinders) is the least important.\n",
    "\n",
    "10. Gradient-Boosted Trees\n",
    "\n",
    "The second ensemble model you'll be looking at is Gradient-Boosted Trees. Again the aim is to build a collection of diverse models, but the approach is slightly different. Rather than building a set of trees that operate in parallel, now we build trees which work in series. The boosting algorithm works iteratively. First build a decision tree and add to the ensemble. Then use the ensemble to make predictions on the training data. Compare the predicted labels to the known labels. Now identify training instances where predictions were incorrect. Return to the start and train another tree which focuses on improving the incorrect predictions. As trees are added to the ensemble its predictions improve because each new tree focuses on correcting the shortcomings of the preceding trees.\n",
    "\n",
    "11. Boosting trees\n",
    "\n",
    "The class for the Gradient-Boosted Tree classifier is also found in the classification sub-module. After creating an instance of the class you fit it to the training data.\n",
    "\n",
    "12. Comparing trees\n",
    "\n",
    "You can make an objective comparison between a plain Decision Tree and the two ensemble models by looking at the values of AUC obtained by each of them on the testing data. Both of the ensemble methods score better than the Decision Tree. This is not too surprising since they are significantly more powerful models. It's also worth noting that these results are based on the default parameters for these models. It should be possible to get even better performance by tuning those parameters using cross-validation.\n",
    "\n",
    "13. Ensemble all of the models!\n",
    "\n",
    "In the final set of exercises you'll try out ensemble methods on the flights data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93602b96",
   "metadata": {},
   "source": [
    "### Delayed flights with Gradient-Boosted Trees\n",
    "You've previously built a classifier for flights likely to be delayed using a Decision Tree. In this exercise you'll compare a Decision Tree model to a Gradient-Boosted Trees model.\n",
    "\n",
    "The `flights` data have been randomly split into `flights_train` and `flights_test`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import the classes required to create Decision Tree and Gradient-Boosted Tree classifiers.\n",
    "- Create Decision Tree and Gradient-Boosted Tree classifiers. Train on the training data.\n",
    "- Create an evaluator and calculate AUC on testing data for both classifiers. Which model performs better?\n",
    "- For the Gradient-Boosted Tree classifier print the number of trees and the relative importance of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9a21b011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5246650705347962\n",
      "0.565571990782101\n",
      "[DecisionTreeRegressionModel: uid=dtr_a0b0f3da27b3, depth=5, numNodes=39, numFeatures=1, DecisionTreeRegressionModel: uid=dtr_9e1946100902, depth=5, numNodes=27, numFeatures=1, DecisionTreeRegressionModel: uid=dtr_b6afe141f2a5, depth=5, numNodes=27, numFeatures=1, DecisionTreeRegressionModel: uid=dtr_a06f06f745a7, depth=5, numNodes=27, numFeatures=1, DecisionTreeRegressionModel: uid=dtr_e04254236560, depth=5, numNodes=29, numFeatures=1, DecisionTreeRegressionModel: uid=dtr_9f73b7dc7ce7, depth=5, numNodes=29, numFeatures=1, DecisionTreeRegressionModel: uid=dtr_0ef0d6eceb0c, depth=5, numNodes=29, numFeatures=1, DecisionTreeRegressionModel: uid=dtr_a1a004d77c7a, depth=5, numNodes=29, numFeatures=1, DecisionTreeRegressionModel: uid=dtr_0f827249ea80, depth=5, numNodes=29, numFeatures=1, DecisionTreeRegressionModel: uid=dtr_4a5487e0eec5, depth=5, numNodes=29, numFeatures=1, DecisionTreeRegressionModel: uid=dtr_1a6c2a8fec82, depth=5, numNodes=29, numFeatures=1, DecisionTreeRegressionModel: uid=dtr_4fd5f558162f, depth=5, numNodes=21, numFeatures=1, DecisionTreeRegressionModel: uid=dtr_3ea67974ab7a, depth=5, numNodes=29, numFeatures=1, DecisionTreeRegressionModel: uid=dtr_41abaa20a84e, depth=5, numNodes=29, numFeatures=1, DecisionTreeRegressionModel: uid=dtr_42723f5e8d72, depth=5, numNodes=21, numFeatures=1, DecisionTreeRegressionModel: uid=dtr_027131f601ae, depth=5, numNodes=27, numFeatures=1, DecisionTreeRegressionModel: uid=dtr_adc794f14a33, depth=5, numNodes=29, numFeatures=1, DecisionTreeRegressionModel: uid=dtr_b05934d04ca3, depth=5, numNodes=21, numFeatures=1, DecisionTreeRegressionModel: uid=dtr_4a17ef721273, depth=5, numNodes=29, numFeatures=1, DecisionTreeRegressionModel: uid=dtr_eaaa51be83ee, depth=5, numNodes=21, numFeatures=1]\n",
      "(1,[0],[1.0])\n"
     ]
    }
   ],
   "source": [
    "# Import the classes required\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Create model objects and train on training data\n",
    "tree = DecisionTreeClassifier().fit(flights_train)\n",
    "gbt = GBTClassifier().fit(flights_train)\n",
    "\n",
    "# Compare AUC on testing data\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(evaluator.evaluate(tree.transform(flights_test)))\n",
    "print(evaluator.evaluate(gbt.transform(flights_test)))\n",
    "\n",
    "# Find the number of trees and the relative importance of features\n",
    "print(gbt.trees)\n",
    "print(gbt.featureImportances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecc9293",
   "metadata": {},
   "source": [
    "### Delayed flights with a Random Forest\n",
    "In this exercise you'll bring together cross validation and ensemble methods. You'll be training a Random Forest classifier to predict delayed flights, using cross validation to choose the best values for model parameters.\n",
    "\n",
    "You'll find good values for the following parameters:\n",
    "\n",
    "- `featureSubsetStrategy` — the number of features to consider for splitting at each node and\n",
    "- `maxDepth` — the maximum number of splits along any branch.\n",
    "\n",
    "Unfortunately building this model takes too long, so we won't be running the `.fit()` method on the pipeline.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Create a random forest classifier object.\n",
    "- Create a parameter grid builder object. Add grid points for the `featureSubsetStrategy` and `maxDepth` parameters.\n",
    "- Create binary classification evaluator.\n",
    "- Create a cross-validator object, specifying the estimator, parameter grid and evaluator. Choose 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c491338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dc808f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier\n",
    "forest = RandomForestClassifier()\n",
    "\n",
    "# Create a parameter grid\n",
    "params = ParamGridBuilder() \\\n",
    "            .addGrid(forest.featureSubsetStrategy, ['all', 'onethird', 'sqrt', 'log2']) \\\n",
    "            .addGrid(forest.maxDepth, [2, 5, 10]) \\\n",
    "            .build()\n",
    "\n",
    "# Create a binary classification evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "# Create a cross-validator\n",
    "cv = CrossValidator(estimator = forest, evaluator = evaluator, estimatorParamMaps=params,  numFolds = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acf9c52",
   "metadata": {},
   "source": [
    "### Evaluating Random Forest\n",
    "In this final exercise you'll be evaluating the results of cross-validation on a Random Forest model.\n",
    "\n",
    "The following have already been created:\n",
    "\n",
    "- `cv` - a cross-validator which has already been fit to the training data\n",
    "- `evaluator` — a `BinaryClassificationEvaluator` object and\n",
    "- `flights_test` — the testing data.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Print a list of average AUC metrics across all models in the parameter grid.\n",
    "- Display the average AUC for the best model. This will be the largest AUC in the list.\n",
    "- Print an explanation of the `maxDepth` and `featureSubsetStrategy` parameters for the best model.\n",
    "- Display the AUC for the best model predictions on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "48d18a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = cv.fit(flights_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5f363e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5589995799344675, 0.568171669034667, 0.5675617404647673, 0.5589995799344675, 0.568171669034667, 0.5675617404647673, 0.5589995799344675, 0.568171669034667, 0.5675617404647673, 0.5589995799344675, 0.568171669034667, 0.5675617404647673]\n",
      "0.568171669034667\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5, current: 5)\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: auto, current: all)\n",
      "0.5615587407173772\n"
     ]
    }
   ],
   "source": [
    "# Average AUC for each parameter combination in grid\n",
    "print(cv.avgMetrics)\n",
    "\n",
    "# Average AUC for the best model\n",
    "print(max(cv.avgMetrics))\n",
    "\n",
    "# What's the optimal parameter value for maxDepth?\n",
    "print(cv.bestModel.explainParam('maxDepth'))\n",
    "# What's the optimal parameter value for featureSubsetStrategy?\n",
    "print(cv.bestModel.explainParam('featureSubsetStrategy'))\n",
    "\n",
    "# AUC for best model on testing data\n",
    "print(evaluator.evaluate(cv.bestModel.transform(flights_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06bf015",
   "metadata": {},
   "source": [
    "# Closing thoughts\n",
    "\n",
    "1. Closing thoughts\n",
    "\n",
    "Congratulations on completing this course on Machine Learning with Apache Spark. You have covered a lot of ground, reviewing some Machine Learning fundamentals and seeing how they can be applied to large datasets, using Spark for distributed computing.\n",
    "\n",
    "2. Things you've learned\n",
    "\n",
    "You learned how to load data into Spark and then perform a variety of operations on those data. Specifically, you learned basic column manipulation on DataFrames, how to deal with text data, bucketing continuous data and one-hot encoding categorical data. You then delved into two types of classifiers, Decision Trees and Logistic Regression, in the process building a robust spam classifier. You also learned about partitioning your data and how to use testing data and a selection of metrics to evaluate a model. Next you learned about regression, starting with a simple linear regression model and progressing to penalized regression, which allowed you to build a model using only the most relevant predictors. You learned about pipelines and how they can make your Spark code cleaner and easier to maintain. This led naturally into using cross-validation and grid search to derive more robust model metrics and use them to select good model parameters. Finally you encountered two forms of ensemble models.\n",
    "\n",
    "3. Learning more\n",
    "\n",
    "Of course, there are many topics that were not covered in this course. If you want to dig deeper then consult the excellent and extensive online documentation. Importantly you can find instructions for setting up and securing a Spark cluster.\n",
    "\n",
    "4. Congratulations!\n",
    "\n",
    "Now go and use what you've learned to solve challenging and interesting big data problems in the real world!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
