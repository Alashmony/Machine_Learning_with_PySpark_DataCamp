{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e00f1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width : 99% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.output_result {width : 99% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.jp-Notebook {--jp-notebook-max-width: 99%;}/style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container {width : 99% !important;}</style>\"))\n",
    "display(HTML(\"<style>.output_result {width : 99% !important;}</style>\"))\n",
    "display(HTML(\"<style>.jp-Notebook {--jp-notebook-max-width: 99%;}/style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f727b75-72b0-4c24-9d01-dbbae4653a6f",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1a0fd5-1d52-449a-8800-22803b063bb9",
   "metadata": {},
   "source": [
    "## Machine Learning & Spark\n",
    "1. Machine Learning & Spark\n",
    "\n",
    "Hi! Welcome to the course on Machine Learning with Apache Spark, in which you will learn how to build Machine Learning models on large data sets using distributed computing techniques. Let's start with some fundamental concepts.\n",
    "\n",
    "2. Building the perfect waffle (an analogy)\n",
    "\n",
    "Suppose you wanted to teach a computer how to make waffles. You could find a good recipe and then give the computer explicit instructions about ingredients and proportions. Alternatively, you could present the computer with a selection of different waffle recipes and let it figure out the ingredients and proportions for the best recipe. The second approach is how Machine Learning works: the computer literally learns from examples.\n",
    "\n",
    "3. Regression & classification\n",
    "\n",
    "Machine Learning problems are generally less esoteric than finding the perfect waffle recipe. The most common problems apply either Regression or Classification. A regression model learns to predict a number. For example, when making waffles, how much flour should be used for a particular amount of sugar? A classification model, on the other hand, predicts a discrete or categorical value. For example, is a recipe calling for a particular amount of sugar and salt more likely to be for waffles or cupcakes?\n",
    "\n",
    "4. Data in RAM\n",
    "\n",
    "The performance of a Machine Learning model depends on data. In general, more data is a good thing. If an algorithm is able to train on a larger set of data, then its ability to generalize to new data will inevitably improve. However, there are some practical constraints. If the data can fit entirely into RAM then the algorithm can operate efficiently. What happens when those data no longer fit into memory?\n",
    "\n",
    "5. Data exceeds RAM\n",
    "\n",
    "The computer will start to use *virtual memory* and data will be *paged* back and forth between RAM and disk. Relative to RAM access, retrieving data from disk is slow. As the size of the data grows, paging becomes more intense and the computer begins to spend more and more time waiting for data. Performance plummets.\n",
    "\n",
    "6. Data distributed across a cluster\n",
    "\n",
    "How then do we deal with truly large datasets? One option is to distribute the problem across multiple computers in a cluster. Rather than trying to handle a large dataset on a single machine, it's divided up into partitions which are processed separately. Ideally each data partition can fit into RAM on a single computer in the cluster. This is the approach used by Spark.\n",
    "\n",
    "7. What is Spark?\n",
    "\n",
    "Spark is a general purpose framework for cluster computing. It is popular for two main reasons: 1. it's generally much faster than other Big Data technologies like Hadoop, because it does most processing in memory and 2. it has a developer-friendly interface which hides much of the complexity of distributed computing.\n",
    "\n",
    "8. Components: nodes\n",
    "\n",
    "Let's review the components of a Spark cluster. The cluster itself consists of one or more nodes. Each node is a computer with CPU, RAM and physical storage.\n",
    "\n",
    "9. Components: cluster manager\n",
    "\n",
    "A cluster manager allocates resources and coordinates activity across the cluster.\n",
    "\n",
    "10. Components: driver\n",
    "\n",
    "Every application running on the Spark cluster has a driver program. Using the Spark API, the driver communicates with the cluster manager, which in turn distributes work to the nodes.\n",
    "\n",
    "11. Components: executors\n",
    "\n",
    "On each node Spark launches an executor process which persists for the duration of the application. Work is divided up into tasks, which are simply units of computation. The executors run tasks in multiple threads across the cores in a node. When working with Spark you normally don't need to worry *too* much about the details of the cluster. Spark sets up all of that infrastructure for you and handles all interactions within the cluster. However, it's still useful to know how it works under the hood.\n",
    "\n",
    "12. Onward!\n",
    "\n",
    "You now have a basic understanding of the principles of Machine Learning and distributed computing with Spark. Next we'll learn how to connect to a Spark cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676fee60-2721-422f-bde5-f2d260023df0",
   "metadata": {},
   "source": [
    "### Characteristics of Spark\n",
    "Spark is currently the most popular technology for processing large quantities of data. Not only is it able to handle enormous data volumes, but it does so very efficiently too! Also, unlike some other distributed computing technologies, developing with Spark is a pleasure.\n",
    "\n",
    "Which of these describes Spark?\n",
    "\n",
    "Answer the question\n",
    "\n",
    "\n",
    "- Spark is a framework for cluster computing.\n",
    "\n",
    "- Spark does most processing in memory.\n",
    "\n",
    "- Spark has a high-level API, which conceals a lot of complexity.\n",
    "\n",
    "- **All of the above.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f6273-f177-4e3d-bbda-b26b1646122f",
   "metadata": {},
   "source": [
    "### Components in a Spark Cluster\n",
    "Spark is a distributed computing platform. It achieves efficiency by distributing data and computation across a cluster of computers.\n",
    "\n",
    "A Spark cluster consists of a number of hardware and software components which work together.\n",
    "\n",
    "Which of these is not part of a Spark cluster?\n",
    "\n",
    "Answer the question\n",
    "\n",
    "- One or more nodes\n",
    "\n",
    "- A cluster manager\n",
    "\n",
    "- **A load balancer**\n",
    "\n",
    "- Executors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6af067-2fc8-42fc-914d-89a1e9f2e956",
   "metadata": {},
   "source": [
    "## Connecting to Spark\n",
    "1. Connecting to Spark\n",
    "\n",
    "The previous lesson was high level overviews of Machine Learning and Spark. In this lesson you'll review the process of connecting to Spark.\n",
    "\n",
    "2. Interacting with Spark\n",
    "\n",
    "The connection with Spark is established by the driver, which can be written in either Java, Scala, Python or R. Each of these languages has advantages and disadvantages. Java is relatively verbose, requiring a lot of code to accomplish even simple tasks. By contrast, Scala, Python and R, are high-level languages which can accomplish much with only a small amount of code. They also offer a REPL, or Read-Evaluate-Print loop, which is crucial for interactive development. You'll be using Python.\n",
    "\n",
    "3. Importing pyspark\n",
    "\n",
    "Python doesn't talk natively to Spark, so we'll kick off by importing the pyspark module, which makes Spark functionality available in the Python interpreter. Spark is under vigorous development. Because the interface is evolving it's important to know what version you're working with. We'll be using version 2.4.1, which was released in March 2019.\n",
    "\n",
    "4. Sub-modules\n",
    "\n",
    "In addition to the main pyspark module, there are a few sub-modules which implement different aspects of the Spark interface. There are two versions of Spark Machine Learning: mllib, which uses an unstructured representation of data in RDDs and has been deprecated, and ml which is based on a structured, tabular representation of data in DataFrames. We'll be using the latter.\n",
    "\n",
    "5. Spark URL\n",
    "\n",
    "With the pyspark module loaded, you are able to connect to Spark. The next thing you need to do is tell Spark where the cluster is located. Here there are two options. You can either connect to a remote cluster, in which case you need to specify a Spark URL, which gives the network location of the cluster's master node. The URL is composed of an IP address or DNS name and a port number. The default port for Spark is 7077, but this must still be explicitly specified. When you're figuring out how Spark works, the infrastructure of a distributed cluster can get in the way. That's why it's useful to create a local cluster, where everything happens on a single computer. This is the setup that you're going to use throughout this course. For a local cluster, you need only specify \"local\" and, optionally, the number of cores to use. By default, a local cluster will run on a single core. Alternatively, you can give a specific number of cores or simply use the wildcard to choose all available cores.\n",
    "\n",
    "6. Creating a SparkSession\n",
    "\n",
    "You connect to Spark by creating a SparkSession object. The SparkSession class is found in the pyspark.sql sub-module. You specify the location of the cluster using the master() method. Optionally you can assign a name to the application using the appName() method. Finally you call the getOrCreate() method, which will either create a new session object or return an existing object. Once the session has been created you are able to interact with Spark. Finally, although it's possible for multiple SparkSessions to co-exist, it's good practice to stop the SparkSession when you're done.\n",
    "\n",
    "7. Let's connect to Spark!\n",
    "\n",
    "Great! Let's connect to Spark!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea457285-eed7-4fd5-a3a7-6c4825682f4f",
   "metadata": {},
   "source": [
    "### Location of Spark master\n",
    "Which of the following is not a valid way to specify the location of a Spark cluster?\n",
    "\n",
    "Answer the question\n",
    "\n",
    "- spark://13.59.151.161:7077\n",
    "\n",
    "- spark://ec2-18-188-22-23.us-east-2.compute.amazonaws.com:7077\n",
    "\n",
    "- **spark://18.188.22.23**\n",
    "\n",
    "- local\n",
    "\n",
    "- local[4]\n",
    "\n",
    "- local[*]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2555cbc8-4b07-40d5-bc41-2d5fc2301605",
   "metadata": {},
   "source": [
    "### Creating a SparkSession\n",
    "In this exercise, you'll spin up a local Spark cluster using all available cores. The cluster will be accessible via a `SparkSession` object.\n",
    "\n",
    "The `SparkSession` class has a builder attribute, which is an instance of the Builder class. The Builder class exposes three important methods that let you:\n",
    "\n",
    "- specify the location of the master node;\n",
    "- name the application (optional); and\n",
    "- retrieve an existing `SparkSession` or, if there is none, create a new one.\n",
    "- The `SparkSession` class has a version attribute that gives the version of Spark. Note: The version can also be accessed via the `__version__` attribute on the `pyspark` module.\n",
    "\n",
    "Find out more about `SparkSession` [here](https://spark.apache.org/docs/2.3.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession).\n",
    "\n",
    "Once you are finished with the cluster, it's a good idea to shut it down, which will free up its resources, making them available for other processes.\n",
    "\n",
    "**Note**:: You might find it useful to review the slides from the lessons in the Slides panel next to the *IPython Shell*.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import the `SparkSession` class from `pyspark.sql`.\n",
    "- Create a `SparkSession` object connected to a local cluster. Use all available cores. Name the application `'test'`.\n",
    "- Use the version attribute on the `SparkSession` object to retrieve the version of Spark running on the cluster. **Note**: The version might be different from the one that's used in the presentation (it gets updated from time to time).\n",
    "- Shut down the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "969ab500-007e-40f2-a4fb-646defff53be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/05 14:04:41 WARN Utils: Your hostname, Alashmony-Lenovo-Z51-70 resolves to a loopback address: 127.0.1.1; using 192.168.1.182 instead (on interface wlp3s0)\n",
      "23/10/05 14:04:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/05 14:04:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.0\n"
     ]
    }
   ],
   "source": [
    "# Import the SparkSession class\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession object\n",
    "spark = SparkSession.builder \\\n",
    "                    .master('local[*]') \\\n",
    "                    .appName('test') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# What version of Spark?\n",
    "print(spark.version)\n",
    "\n",
    "# Terminate the cluster\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1790853-296d-4b44-9910-88aee24c2465",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "1. Loading Data\n",
    "\n",
    "In this lesson you'll look at how to read data into Spark.\n",
    "\n",
    "3. DataFrames: A refresher\n",
    "\n",
    "Spark represents tabular data using the DataFrame class. The data are captured as rows (or \"records\"), each of which is broken down into one or more columns (or \"fields\"). Every column has a name and a specific data type. Some selected methods and attributes of the DataFrame class are listed here. The count() method gives the number of rows. The show() method will display a subset of rows. The printSchema() method and the dtypes attribute give different views on column types. This is really scratching the surface of what's possible with a DataFrame. You can find out more by consulting the extensive documentation.\n",
    "\n",
    "4. CSV data for cars\n",
    "\n",
    "CSV is a common format for storing tabular data. For illustration we'll be using a CSV file with characteristics for a selection of motor vehicles. Each line in a CSV file is a new record and within each record, fields are separated by a delimiter character, which is normally a comma. The first line is an optional header record which gives column names.\n",
    "\n",
    "5. Reading data from CSV\n",
    "\n",
    "Our session object has a \"read\" attribute which, in turn, has a csv() method which reads data from a CSV file and returns a DataFrame. The csv() method has one mandatory argument, the path to the CSV file. There are a number of optional arguments. We'll take a quick look at some of the most important ones. The header argument specifies whether or not there is a header record. The sep argument gives the field separator, which is a comma by default. There are two arguments which pertain to column data types, schema and inferSchema. Finally, the nullValue argument gives the placeholder used to indicate missing data. Let's take a look at the data we've just loaded.\n",
    "\n",
    "6. Peek at the data\n",
    "\n",
    "Using the show() method we can take a look at a slice of the DataFrame. The csv() method has split the data into rows and columns and picked up the column names from the header record. Looks great, doesn't it? Unfortunately there's a small snag. Before we unravel that snag, it's important to note that the first value in the cylinder column is not a number. It's the string \"NA\" which indicates missing data.\n",
    "\n",
    "7. Check column types\n",
    "\n",
    "If you check the column data types then you'll find that they are all strings. That doesn't make sense since the last six columns are clearly numbers! However, this is the expected behavior: the csv() method treats all columns as strings by default. You need to do a little more work to get the correct column types. There are two ways that you can do this: infer the column types from the data or manually specify the types.\n",
    "\n",
    "8. Inferring column types from data\n",
    "\n",
    "It's possible to reasonably deduce the column types by setting the inferSchema argument to True. There is a price to pay though: Spark needs to make an extra pass over the data to figure out the column types before reading the data. If the data file is big then this will increase load time notably. Using this approach all of the column types are correctly identified except for cylinder. Why? The first value in this column is \"NA\", so Spark thinks that the column contains strings.\n",
    "\n",
    "9. Dealing with missing data\n",
    "\n",
    "Missing data in CSV files are normally represented by a placeholder like the \"NA\" string. You can use the nullValue argument to specify the placeholder. It's always a good idea to explicitly define the missing data placeholder. The nullValue argument is case sensitive, so it's important to provide it in exactly the same form as it appears in the data file.\n",
    "\n",
    "10. Specify column types\n",
    "\n",
    "If inferring column type is not successful then you have the option of specifying the type of each column in an explicit schema. This also makes it possible to choose alternative column names.\n",
    "\n",
    "11. Final cars data\n",
    "\n",
    "This is what the final cars data look like. Note that the missing value at the top of the cylinders column is indicated by the special null constant.\n",
    "\n",
    "12. Let's load some data!\n",
    "\n",
    "You're ready to use what you've learned to load data from CSV files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78a66096",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.182:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=test>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a924882e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/05 14:04:46 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.182:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9e964b5090>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc).builder\\\n",
    ".master(\"local[2]\")\\\n",
    ".appName(\"MLSpark\")\\\n",
    ".getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99612b64-9d8e-4614-9c8b-2d5c180a1fb6",
   "metadata": {},
   "source": [
    "### Loading flights data\n",
    "In this exercise, you're going to load some airline flight data from a CSV file. To ensure that the exercise runs quickly these data have been trimmed down to only 50 000 records. You can get a larger dataset in the same format [here](https://assets.datacamp.com/production/repositories/3918/datasets/e1c1a03124fb2199743429e9b7927df18da3eacf/flights-larger.csv).\n",
    "\n",
    "Notes on CSV format:\n",
    "\n",
    "- fields are separated by a comma (this is the default separator) and\n",
    "- missing data are denoted by the string 'NA'.\n",
    "\n",
    "**Data dictionary**:\n",
    "\n",
    "- `mon` — month (integer between 1 and 12)\n",
    "- `dom` — day of month (integer between 1 and 31)\n",
    "- `dow` — day of week (integer; 1 = Monday and 7 = Sunday)\n",
    "- `carrier` — carrier (IATA code)\n",
    "- `flight` — flight number\n",
    "- `org` — origin airport (IATA code)\n",
    "- `mile` — distance (miles)\n",
    "- `depart` — departure time (decimal hour)\n",
    "- `duration` — expected duration (minutes)\n",
    "- `delay` — delay (minutes)\n",
    "\n",
    "`pyspark` has been imported for you and the session has been initialized.\n",
    "\n",
    "Note: The data have been aggressively down-sampled.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Read data from a CSV file called `'flights.csv'`. Assign data types to columns automatically. Deal with missing data.\n",
    "- How many records are in the data?\n",
    "- Take a look at the first five records.\n",
    "- What data types have been assigned to the columns? Do these look correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29a5010f-81b1-4d9f-81db-424d09f6e296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contain 50000 records.\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| null|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|\n",
      "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|\n",
      "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|\n",
      "|  4|  2|  5|     AA|   325|ORD| 258|  8.92|      65| null|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "[('mon', 'int'), ('dom', 'int'), ('dow', 'int'), ('carrier', 'string'), ('flight', 'int'), ('org', 'string'), ('mile', 'int'), ('depart', 'double'), ('duration', 'int'), ('delay', 'int')]\n"
     ]
    }
   ],
   "source": [
    "# Read data from CSV file\n",
    "flights = spark.read.csv('flights.csv',\n",
    "                         sep=',',\n",
    "                         header=True,\n",
    "                         inferSchema=True,\n",
    "                         nullValue='NA')\n",
    "\n",
    "# Get number of records\n",
    "print(\"The data contain %d records.\" % flights.count())\n",
    "\n",
    "# View the first five records\n",
    "flights.show(5)\n",
    "\n",
    "# Check column data types\n",
    "print(flights.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af462366-2564-4c6a-b9a2-0baa0f2feac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contain 275000 records.\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "| 10| 10|  1|     OO|  5836|ORD| 157|  8.18|      51|   27|\n",
      "|  1|  4|  1|     OO|  5866|ORD| 466|  15.5|     102| null|\n",
      "| 11| 22|  1|     OO|  6016|ORD| 738|  7.17|     127|  -19|\n",
      "|  2| 14|  5|     B6|   199|JFK|2248| 21.17|     365|   60|\n",
      "|  5| 25|  3|     WN|  1675|SJC| 386| 12.92|      85|   22|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "[('mon', 'int'), ('dom', 'int'), ('dow', 'int'), ('carrier', 'string'), ('flight', 'int'), ('org', 'string'), ('mile', 'int'), ('depart', 'double'), ('duration', 'int'), ('delay', 'int')]\n"
     ]
    }
   ],
   "source": [
    "# Read data from CSV file\n",
    "flights_full = spark.read.csv('flights-larger.csv',\n",
    "                         sep=',',\n",
    "                         header=True,\n",
    "                         inferSchema=True,\n",
    "                         nullValue='NA')\n",
    "\n",
    "# Get number of records\n",
    "print(\"The data contain %d records.\" % flights_full.count())\n",
    "\n",
    "# View the first five records\n",
    "flights_full.show(5)\n",
    "\n",
    "# Check column data types\n",
    "print(flights_full.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd05275-f505-4bf8-b86a-2d9315a8ffeb",
   "metadata": {},
   "source": [
    "### Loading SMS spam data\n",
    "You've seen that it's possible to infer data types directly from the data. Sometimes it's convenient to have direct control over the column types. You do this by defining an explicit schema.\n",
    "\n",
    "The file `sms.csv` contains a selection of SMS messages which have been classified as either `'spam'` or `'ham'`. These data have been adapted from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). There are a total of 5574 SMS, of which 747 have been labeled as spam.\n",
    "\n",
    "**Notes on CSV format**:\n",
    "\n",
    "- no header record and\n",
    "- fields are separated by a semicolon (this is not the default separator).\n",
    "\n",
    "**Data dictionary**:\n",
    "\n",
    "- `id` — record identifier\n",
    "- `text` — content of SMS message\n",
    "- `label` — spam or ham (integer; 0 = ham and 1 = spam)\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Specify the data schema, giving columns names (`\"id\"`, `\"text\"`, and `\"label\"`) and column types.\n",
    "- Read data from a delimited file called `\"sms.csv\"`.\n",
    "- Print the schema for the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eef7bb2-fb63-4d06-9f41-b8acc9e9ec4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Specify column names and types\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"label\", IntegerType())\n",
    "])\n",
    "\n",
    "# Load data from a delimited file\n",
    "sms = spark.read.csv('sms.csv', sep=';', header=False, schema=schema)\n",
    "\n",
    "# Print schema of DataFrame\n",
    "sms.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651dbc48-1f6d-4553-a549-428913584829",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159bd702-ca88-438a-8de9-11b1af62f280",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "1. Data Preparation\n",
    "\n",
    "In this lesson you are going to learn how to prepare data for building a Machine Learning model.\n",
    "\n",
    "2. Do you need all of those columns?\n",
    "\n",
    "You'll be working with the cars data again. This is what the data look like at present. There are columns for the maker and model, the origin (either USA or non-USA), the type, number of cylinders, engine size, weight, length, RPM and fuel consumption. The models that you'll be building will depend on the physical characteristics of the cars rather than the model names or manufacturers, so you'll remove the corresponding columns from the data.\n",
    "\n",
    "3. Dropping columns\n",
    "\n",
    "There are two approaches to doing this: either you can drop() the columns that you don't want or you can select() the fields which you do want to retain. Either way, the resulting data does not include those columns.\n",
    "\n",
    "4. Filtering out missing data\n",
    "\n",
    "Earlier you saw that there is a missing value in the cylinders column. Let's check to see how many other missing values there are. You'll use the filter() method and provide a logical predicate using SQL syntax which identifies NULL values. Then the count() method tells you how many records there are remaining. Just one. In this case it makes sense to simply remove the record with the missing value. There are a couple of ways that you could to do this. You could use the filter() method again with a different predicate. Or you could take a more aggressive approach and use the dropna() method to drop all records with missing values in any column. However, this should be done with care because it could result in the loss of a lot of otherwise useful data. You've now stripped down the data to what's needed to build a model.\n",
    "\n",
    "5. Mutating columns\n",
    "\n",
    "At present the weight and length columns are in units of pounds and inches respectively. You'll use the withColumn() method to create a new mass column in units of kilograms. The round() function is used to limit the precision of the result. You can also use the withColumn() method to replace the existing length column with values in meters. You now have mass and length in metric units.\n",
    "\n",
    "6. Indexing categorical data\n",
    "\n",
    "The type column consists of strings which represent six categories of vehicle type. You'll need to transform those strings into numbers. You do this using an instance of the StringIndexer class. In the constructor you provide the name of the string input column and a name for the new output column to be created. The indexer is first fit to the data, creating a StringIndexerModel. During the fitting process the distinct string values are identified and an index is assigned to each value. The model is then used to transform the data, creating a new column with the index values. By default the index values are assigned according to the descending relative frequency of each of the string values. Midsize is most common, so it gets an index of zero. Small is next most common, so its index is one. And so on. It's possible to choose different strategies for assigning index values by specifying the stringOrderType argument. Rather than using frequency of occurrence, strings can be ordered alphabetically. It's also possible to choose between ascending and descending order.\n",
    "\n",
    "7. Indexing country of origin\n",
    "\n",
    "You'll be building a classifier to predict whether or not a car was manufactured in the USA. So the origin column also needs to be converted from strings into numbers.\n",
    "\n",
    "8. Assembling columns\n",
    "\n",
    "The final step in preparing the cars data is to consolidate the various input columns into a single column. This is necessary because the Machine Learning algorithms in Spark operate on a single vector of predictors, although each element in that vector may consist of multiple values. To illustrate the process you'll start with just a pair of features, cylinders and size. First you create an instance of the VectorAssembler class, providing it with the names of the columns that you want to consolidate and the name of the new output column. The assembler is then used to transform the data. Taking a look at the relevant columns you see that the new \"features\" column consists of values from the cylinders and size columns consolidated into a vector. Ultimately you are going to assemble all of the predictors into a single column.\n",
    "\n",
    "9. Let's practice!\n",
    "\n",
    "Let's try out what we have learned on the SMS and flights data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f384d824-cfe0-4b48-97b3-6beebd12e9c5",
   "metadata": {},
   "source": [
    "### Removing columns and rows\n",
    "You previously loaded airline flight data from a CSV file. You're going to develop a model which will predict whether or not a given flight will be delayed.\n",
    "\n",
    "In this exercise, you need to trim those data down by:\n",
    "\n",
    "- removing an uninformative column and\n",
    "- removing rows that do not have information about whether or not a flight was delayed.\n",
    "\n",
    "The data are available as `flights`.\n",
    "\n",
    "**Note**:: You might find it helpful to revise the slides from the lessons in the Slides panel next to the IPython Shell.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Remove the `flight` column.\n",
    "- Find out how many records have missing values in the `delay` column.\n",
    "- Remove records with missing values in the `delay` column.\n",
    "- Remove records with missing values in any column and get the number of remaining rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53fcbbd8-5ae9-4023-ad89-a29ed61c087c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47022\n"
     ]
    }
   ],
   "source": [
    "# Remove the 'flight' column\n",
    "flights_drop_column = flights.drop('flight')\n",
    "\n",
    "# Number of records with missing 'delay' values\n",
    "flights_drop_column.filter('delay IS NULL').count()\n",
    "\n",
    "# Remove records with missing 'delay' values\n",
    "flights_valid_delay = flights_drop_column.filter('delay IS NOT NULL')\n",
    "\n",
    "# Remove records with missing values in any column and get the number of remaining rows\n",
    "flights_none_missing = flights_valid_delay.dropna()\n",
    "print(flights_none_missing.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa678017-0113-455f-a308-2ea3ad4bfd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2978\n"
     ]
    }
   ],
   "source": [
    "print(flights_drop_column.filter('delay IS NULL').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1238d6b-fa58-4ce3-addb-7cf2bd57f287",
   "metadata": {},
   "source": [
    "### Column manipulation\n",
    "The Federal Aviation Administration (FAA) considers a flight to be \"delayed\" when it arrives 15 minutes or more after its scheduled time.\n",
    "\n",
    "The next step of preparing the flight data has two parts:\n",
    "\n",
    "- convert the units of distance, replacing the `mile` column with a `km` column; and\n",
    "- create a Boolean column indicating whether or not a flight was delayed.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import a function which will allow you to round a number to a specific number of decimal places.\n",
    "- Derive a new `km` column from the `mile` column, rounding to zero decimal places. One mile is 1.60934 km.\n",
    "- Remove the `mile` column.\n",
    "- Create a label column with a value of `1` indicating the delay was 15 minutes or more and `0` otherwise. Think carefully about the logical condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ecd6d16-82d4-4059-896a-c53ffd3a8e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| null|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|\n",
      "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|\n",
      "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|\n",
      "|  4|  2|  5|     AA|   325|ORD| 258|  8.92|      65| null|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4d0a88b-3f65-499b-a427-63445da6a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_backup = flights\n",
    "flights = flights_none_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "666e39d3-9a57-4646-83fe-197db25f3762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the required function\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Convert 'mile' to 'km' and drop 'mile' column (1 mile is equivalent to 1.60934 km)\n",
    "flights_km = flights.withColumn('km', round(flights['mile'] * 1.60934, 0)) \\\n",
    "                    .drop('mile')\n",
    "\n",
    "# Create 'label' column indicating whether flight delayed (1) or not (0)\n",
    "flights_km = flights_km.withColumn('label', (flights_km.delay >= 15).cast('integer'))\n",
    "\n",
    "# Check first five records\n",
    "flights_km.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41db3910-17eb-4e53-ac69-23e23d1738f7",
   "metadata": {},
   "source": [
    "### Categorical columns\n",
    "In the flights data there are two columns, `carrier` and `org`, which hold categorical data. You need to transform those columns into indexed numerical values.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import the appropriate class and create an indexer object to transform the `carrier` column from a string to an numeric index.\n",
    "- Prepare the indexer object on the flight data.\n",
    "- Use the prepared indexer to create the numeric index column.\n",
    "- Repeat the process for the org column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b4be0b8-a81f-4fde-a455-98506f1c6ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_backup2 = flights\n",
    "flights = flights_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6adc4e9e-b8b6-4c92-84cb-2793e217c580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|        0.0|    0.0|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|        0.0|    1.0|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|        1.0|    0.0|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|        0.0|    1.0|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|        1.0|    0.0|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Create an indexer\n",
    "indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')\n",
    "\n",
    "# Indexer identifies categories in the data\n",
    "indexer_model = indexer.fit(flights)\n",
    "\n",
    "# Indexer creates a new column with numeric index values\n",
    "flights_indexed = indexer_model.transform(flights)\n",
    "\n",
    "# Repeat the process for the other categorical feature\n",
    "flights_indexed = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights_indexed).transform(flights_indexed)\n",
    "flights_indexed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0165f4cc-2b7b-4bf9-9daf-a2bfab761528",
   "metadata": {},
   "source": [
    "### Assembling columns\n",
    "The final stage of data preparation is to consolidate all of the predictor columns into a single column.\n",
    "\n",
    "An updated version of the `flights` data, which takes into account all of the changes from the previous few exercises, has the following predictor columns:\n",
    "\n",
    "`mon`, `dom` and `dow`\n",
    "- `carrier_idx` (indexed value from carrier)\n",
    "- `org_idx` (indexed value from org)\n",
    "- `km`\n",
    "- `depart`\n",
    "- `duration`\n",
    "\n",
    "Note: The `truncate=False` argument to the `show()` method prevents data being truncated in the output.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import the class which will assemble the predictors.\n",
    "- Create an assembler object that will allow you to merge the predictors columns into a single column.\n",
    "- Use the assembler to generate a new consolidated column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be7c60c7-7ac9-4118-89bc-7d436767e1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|        0.0|    0.0|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|        0.0|    1.0|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|        1.0|    0.0|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_backup3 = flights\n",
    "flights = flights_indexed\n",
    "flights.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e78676fb-0aca-4413-a5c3-27f4e12ba217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-----+\n",
      "|features                                 |delay|\n",
      "+-----------------------------------------+-----+\n",
      "|[0.0,22.0,2.0,0.0,0.0,509.0,16.33,82.0]  |30   |\n",
      "|[2.0,20.0,4.0,0.0,1.0,542.0,6.17,82.0]   |-8   |\n",
      "|[9.0,13.0,1.0,1.0,0.0,1989.0,10.33,195.0]|-5   |\n",
      "|[5.0,2.0,1.0,0.0,1.0,885.0,7.98,102.0]   |2    |\n",
      "|[7.0,2.0,6.0,1.0,0.0,1180.0,10.83,135.0] |54   |\n",
      "+-----------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary class\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=['mon', 'dom', 'dow',\n",
    "    'carrier_idx', 'org_idx', 'km', 'depart', 'duration'\n",
    "], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "flights_assembled = assembler.transform(flights)\n",
    "\n",
    "# Check the resulting column\n",
    "flights_assembled.select('features', 'delay').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585279d9-136f-4322-b4c5-1b1e731a101d",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "1. Decision Tree\n",
    "\n",
    "Your first Machine Learning model will be a Decision Tree. This is probably the most intuitive model, so it seems like a good place to start.\n",
    "\n",
    "2. Anatomy of a Decision Tree: Root node\n",
    "\n",
    "A Decision Tree is constructed using an algorithm called \"Recursive Partitioning\". Consider a hypothetical example in which you build a Decision Tree to divide data into two classes, green and blue. You start by putting all of the records into the root node. Suppose that there are more green records than blue, in which case this node will be labelled \"green\". Now from amongst the predictors in the data you need to choose the one that will result in the most informative split of the data into two groups. Ideally you want the groups to be as homogeneous (or \"pure\") as possible: one should be mostly green and the other should be mostly blue.\n",
    "\n",
    "3. Anatomy of a Decision Tree: First split\n",
    "\n",
    "Once you have identified the most informative predictor, you split the data into two sets, labeled \"green\" or \"blue\" according to the dominant class. And this is where the recursion kicks in: you then apply exactly the same procedure on each of the child nodes, selecting the most informative predictor and splitting again.\n",
    "\n",
    "4. Anatomy of a Decision Tree: Second split\n",
    "\n",
    "So, for example, the green node on the left could be split again into two groups.\n",
    "\n",
    "5. Anatomy of a Decision Tree: Third split\n",
    "\n",
    "And the resulting green node could once again be split. The depth of each branch of the tree need not be the same. There are a variety of stopping criteria which can cause splitting to stop along a branch. For example, if the number of records in a node falls below a threshold or the purity of a node is above a threshold, then you might stop splitting. Once you have built the Decision Tree you can use it to make predictions for new data by following the splits from the root node along to the tip of a branch. The label for the final node would then be the prediction for the new data.\n",
    "\n",
    "6. Classifying cars\n",
    "\n",
    "Let's make this more concrete by looking at the cars data. You've transformed the country of origin column into a numeric index called 'label', with zero corresponding to cars manufactured in the USA and one for everything else. The remaining columns have all been consolidated into a column called 'features'. You want to build a Decision Tree which will use \"features\" to predict \"label\".\n",
    "\n",
    "7. Split train/test\n",
    "\n",
    "An important aspect of building a Machine Learning model is being able to assess how well it works. In order to do this we use the randomSplit() method to randomly split our data into two sets, a training set and a testing set. The proportions may vary, but generally you're looking at something like an 80:20 split, which means that the training set ends up having around 4 times as many records as the testing set.\n",
    "\n",
    "8. Build a Decision Tree model\n",
    "\n",
    "Finally the moment has come, you're going to build a Decision Tree. You start by creating a DecisionTreeClassifier() object. The next step is to fit the model to the training data by calling the fit() method.\n",
    "\n",
    "9. Evaluating\n",
    "\n",
    "Now that you've trained the model you can assess how effective it is by making predictions on the test set and comparing the predictions to the known values. The transform() method adds new columns to the DataFrame. The prediction column gives the class assigned by the model. You can compare this directly to the known labels in the testing data. Although the model gets the first example wrong, it's correct for the following four examples. There's also a probability column which gives the probabilities assigned to each of the outcome classes. For the first example, the model predicts that the outcome is 0 with probability 96%.\n",
    "\n",
    "10. Confusion matrix\n",
    "\n",
    "A good way to understand the performance of a model is to create a confusion matrix which gives a breakdown of the model predictions versus the known labels. The confusion matrix consists of four counts which are labelled as follows: - \"positive\" indicates a prediction of 1, while - \"negative\" indicates a prediction of 0 and - \"true\" corresponds to a correct prediction, while - \"false\" designates an incorrect prediction. In this case the true positives and true negatives dominate but the model still makes a number of incorrect predictions. These counts can be used to calculate the accuracy, which is the proportion of correct predictions. For our model the accuracy is 74%.\n",
    "\n",
    "11. Let's build Decision Trees!\n",
    "\n",
    "So, now that you know how to build a Decision Tree model with Spark, you can try that out on the flight data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54149c54-3b22-496c-a208-0a09033fdd7e",
   "metadata": {},
   "source": [
    "### Train/test split\n",
    "To objectively assess a Machine Learning model you need to be able to test it on an independent set of data. You can't use the same data that you used to train the model: of course the model will perform (relatively) well on those data!\n",
    "\n",
    "You will split the data into two components:\n",
    "\n",
    "- training data (used to train the model) and\n",
    "- testing data (used to test the model).\n",
    "\n",
    "**Note**: From here on you'll be working with a smaller subset of the flights data, which just makes the exercises run more quickly.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Randomly split the `flights` data into two sets with 80:20 proportions. For repeatability set a random number seed of 43 for the split.\n",
    "- Check that the training data has roughly 80% of the records from the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5507345-8cfe-4f5e-8aef-b61c5216be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_backup3 = flights\n",
    "flights = flights_assembled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b34f9116-8911-4b3f-9fb2-1f543101af93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8025392369529156\n"
     ]
    }
   ],
   "source": [
    "# Split into training and testing sets in a 80:20 ratio\n",
    "flights_train, flights_test = flights.randomSplit([0.8,0.2], seed = 43)\n",
    "\n",
    "# Check that training set has around 80% of records\n",
    "training_ratio = flights_train.count() / flights.count()\n",
    "print(training_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca0a2c7-f813-4a6d-b78c-5505d1f50b28",
   "metadata": {},
   "source": [
    "### Build a Decision Tree\n",
    "Now that you've split the flights data into training and testing sets, you can use the training set to fit a Decision Tree model.\n",
    "\n",
    "The data are available as `flights_train` and `flights_test`.\n",
    "\n",
    "**NOTE**: It will take a few seconds for the model to train… please be patient!\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import the class for creating a Decision Tree classifier.\n",
    "- Create a classifier object and fit it to the training data.\n",
    "- Make predictions for the testing data and take a look at the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c58f6da-74a0-42a9-b841-98c7568837b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------------------------------------+\n",
      "|label|prediction|probability                            |\n",
      "+-----+----------+---------------------------------------+\n",
      "|1    |0.0       |[0.5329733898958735,0.4670266101041265]|\n",
      "|1    |0.0       |[0.5329733898958735,0.4670266101041265]|\n",
      "|0    |1.0       |[0.3563631693848722,0.6436368306151278]|\n",
      "|1    |1.0       |[0.3563631693848722,0.6436368306151278]|\n",
      "|1    |1.0       |[0.3563631693848722,0.6436368306151278]|\n",
      "+-----+----------+---------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Import the Decision Tree Classifier class\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create a classifier object and fit to the training data\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_model = tree.fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "prediction = tree_model.transform(flights_test)\n",
    "prediction.select('label', 'prediction', 'probability').show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a900e668-85f5-410a-9379-f50df505c17e",
   "metadata": {},
   "source": [
    "### Evaluate the Decision Tree\n",
    "You can assess the quality of your model by evaluating how well it performs on the testing data. Because the model was not trained on these data, this represents an objective assessment of the model.\n",
    "\n",
    "A confusion matrix gives a useful breakdown of predictions versus known values. It has four cells which represent the counts of:\n",
    "\n",
    "- True Negatives (TN) — model predicts negative outcome & known outcome is negative\n",
    "- True Positives (TP) — model predicts positive outcome & known outcome is positive\n",
    "- False Negatives (FN) — model predicts negative outcome but known outcome is positive\n",
    "- False Positives (FP) — model predicts positive outcome but known outcome is negative.\n",
    "These counts (`TN`, `TP`, `FN` and `FP`) should sum to the number of records in the testing data, which is only a subset of the flights data. You can compare to the number of records in the tests data, which is `flights_test.count()`.\n",
    "\n",
    "**Note**: These predictions are made on the testing data, so the counts are smaller than they would have been for predictions on the training data.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Create a confusion matrix by counting the combinations of label and prediction. Display the result.\n",
    "- Count the number of True Negatives, True Positives, False Negatives and False Positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76a4e06d-d796-425e-ab09-282316b09aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 1207|\n",
      "|    0|       0.0| 2371|\n",
      "|    1|       1.0| 3556|\n",
      "|    0|       1.0| 2151|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6383414108777599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a confusion matrix\n",
    "prediction.groupBy('label', 'prediction').count().show()\n",
    "\n",
    "# Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label <> prediction').count()\n",
    "FP = prediction.filter('prediction = 1 AND label <> prediction').count()\n",
    "\n",
    "# Accuracy measures the proportion of correct predictions\n",
    "accuracy = (TP + TN)/ flights_test.count()\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9a925e6-a50b-4d8e-b0cb-5b89c8a81c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3556 2371 2151 1207\n"
     ]
    }
   ],
   "source": [
    "print(TP, TN, FP, FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f08bef-ff8a-40a8-809d-792a483519d5",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "1. Logistic Regression\n",
    "\n",
    "You've learned to build a Decision Tree. But it's good to have options. Logistic Regression is another commonly used classification model.\n",
    "\n",
    "2. Logistic Curve\n",
    "\n",
    "It uses a logistic function to model a binary target, where the target states are usually denoted by 1 and 0 or TRUE and FALSE. The maths of the model are outside the scope of this course, but this is what the logistic function looks like. For a Logistic Regression model the x-axis is a linear combination of predictor variables and the y-axis is the output of the model. Since the value of the logistic function is a number between zero and one, it's often thought of as a probability. In order to translate this number into one or other of the target states it's compared to a threshold, which is normally set at one half.\n",
    "\n",
    "3. Logistic Curve\n",
    "\n",
    "If the number is above the threshold then the predicted state is one.\n",
    "\n",
    "4. Logistic Curve\n",
    "\n",
    "Conversely, if it's below the threshold then the predicted state is zero. The model derives coefficients for each of the numerical predictors. Those coefficients might...\n",
    "\n",
    "5. Logistic Curve\n",
    "\n",
    "shift the curve to the right...\n",
    "\n",
    "6. Logistic Curve\n",
    "\n",
    "or to the left. They might make the transition between states...\n",
    "\n",
    "7. Logistic Curve\n",
    "\n",
    "more gradual...\n",
    "\n",
    "8. Logistic Curve\n",
    "\n",
    "or more rapid. These characteristics are all extracted from the training data and will vary from one set of data to another.\n",
    "\n",
    "9. Cars revisited\n",
    "\n",
    "Let's make this more concrete by returning to the cars data. You'll focus on the numerical predictors for the moment and return to categorical predictors later on. As before you prepare the data by consolidating the predictors into a single column and then randomly splitting the data into training and testing sets.\n",
    "\n",
    "10. Build a Logistic Regression model\n",
    "\n",
    "To build a Logistic Regression model you first need to import the associated class and then create a classifier object. This is then fit to the training data using the fit() method.\n",
    "\n",
    "11. Predictions\n",
    "\n",
    "With a trained model you are able to make predictions on the testing data. As you saw with the Decision Tree, the transform() method adds the prediction and probability columns. The probability column gives the predicted probability of each class, while the prediction column reflects the predicted label, which is derived from the probabilities by applying the threshold mentioned earlier.\n",
    "\n",
    "12. Precision and recall\n",
    "\n",
    "You can assess the quality of the predictions by forming a confusion matrix. The quantities in the cells of the matrix can then be used to form some informative ratios. Recall that a positive prediction indicates that a car is manufactured outside of the USA and that predictions are considered to be true or false depending on whether they are correct or not. Precision is the proportion of positive predictions which are correct. For your model, two thirds of predictions for cars manufactured outside of the USA are correct. Recall is the proportion of positive targets which are correctly predicted. Your model also identifies 80% of cars which are actually manufactured outside of the USA. Bear in mind that these metrics are based on a relatively small testing set.\n",
    "\n",
    "13. Weighted metrics\n",
    "\n",
    "Another way of looking at these ratios is to weight them across the positive and negative predictions. You can do this by creating an evaluator object and then calling the evaluate() method. This method accepts an argument which specifies the required metric. It's possible to request the weighted precision and recall as well as the overall accuracy. It's also possible to get the F1 metric, the harmonic mean of precision and recall, which is generally more robust than the accuracy. All of these metrics have assumed a threshold of one half. What happens if you vary that threshold?\n",
    "\n",
    "14. ROC and AUC\n",
    "\n",
    "A threshold is used to decide whether the number returned by the Logistic Regression model translates into either the positive or the negative class. By default that threshold is set at a half. However, this is not the only choice. Choosing a larger or smaller value for the threshold will affect the performance of the model. The ROC curve plots the true positive rate versus the false positive rate as the threshold increases from zero (top right) to one (bottom left). The AUC summarizes the ROC curve in a single number. It's literally the area under the ROC curve. AUC indicates how well a model performs across all values of the threshold. An ideal model, that performs perfectly regardless of the threshold, would have AUC of 1. In an exercise we'll see how to use another evaluator to calculate the AUC.\n",
    "\n",
    "15. Let's do Logistic Regression!\n",
    "\n",
    "You now know how to build a Logistic Regression model and assess the performance of that model using various metrics. Let's give this a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86b77bc-e3fb-4a0b-8567-4d8df1bbf3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
